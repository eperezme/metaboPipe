The following text is a Git repository with code. The structure of the text are sections that begin with ----, followed by a single line containing the file path and file name, followed by a variable amount of lines containing the file contents. The text representing the Git repository ends when the symbols --END-- are encounted. Any further text beyond --END-- are meant to be interpreted as instructions using the aforementioned Git repository as context.
----
README.md
Shield: [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]

This work is licensed under a
[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].

[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]

[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/
[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png
[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg
----
Work\.Rbuildignore
^renv$
^renv\.lock$
^LICENSE\.md$
^.*\.Rproj$
^\.Rproj\.user$

----
Work\.Rprofile
source("renv/activate.R")

----
Work\.gitignore
inst/doc

*.RData
*.RDataTmp
*.Rhistory

_targets/
_targets_r/
results/
Shiny/_targets
Shiny/Output


----
Work\.renvignore
# Ignore Training folder
/Training/

# Ignore GenerateTargets.R
R/GenerateTargets.R
----
Work\DESCRIPTION
Package: metaboPipe
Title: Create a pipeline for metabolomics data analysis
Version: 0.0.0.9000
Authors@R: 
    person("Eduard", "Perez Mendez", , "eperezme@eperezme.com", role = c("aut", "cre"),
           comment = c(ORCID = "0000-0001-6473-591X"))
Description: The package provides a pipeline for metabolomics data analysis. It includes functions for data pre-processing like filtering of missing values and outliers, normalization, missing value imputation and batch correction. The pipeline is implemented using the 'targets' package.
License: CC BY NC SA 4.0
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.1
Suggests: 
    knitr,
    rmarkdown
VignetteBuilder: knitr
Imports: 
    caret,
    impute,
    imputeLCMD,
    missForest,
    pcaMethods,
    structToolbox,
    SummarizedExperiment,
    VIM,
    tidyverse,
    MetaboAnalystR (>= 4.0.0),
    tinytex,
    HotellingEllipse,
    ggforce,
    tools,
    cowplot,
    targets,
    tarchetypes,
    crew,
    pmp,
    fst,
    shiny
Remotes: 
    xia-lab/MetaboAnalystR
Depends: 
    R (>= 2.10)
LazyData: true

----
Work\NAMESPACE
# Generated by roxygen2: do not edit by hand

export(MetaboAnalyst_load_data)
export(ba_plot)
export(batch_correct)
export(batch_plot)
export(createExperiment)
export(create_pipeline)
export(data.extract)
export(data.modify)
export(distribution_boxplot)
export(exportData)
export(export_data)
export(extract_names)
export(factorize_cols)
export(filter_MV)
export(filter_blanks)
export(filter_outliers)
export(filter_step)
export(impute)
export(impute_QRILC)
export(impute_RF)
export(impute_SVD)
export(impute_bpca)
export(impute_kNN)
export(impute_mean)
export(impute_median)
export(impute_ppca)
export(impute_warper)
export(load_data)
export(metaboNorm)
export(missing_values_plot)
export(normalize)
export(normalize_csn)
export(normalize_metab)
export(normalize_pqn)
export(normalize_vln)
export(pipePilers)
export(plot_boxplots)
export(plot_density_single_with_legend)
export(plot_heatmap)
export(plot_hotelling_obs)
export(plot_hotelling_pca)
export(plot_outliers)
export(plot_pca)
export(sample.data.extract)
export(sample.data.modify)
export(save_metabo)
export(save_plot)
export(sort_by_sample_id)
export(toMetaboAnalyst)
export(variable.data.extract)
export(variable.data.modify)
export(warper_batch_correction)
export(warper_createExperiment)
export(warper_factor_sample_col)
export(zero_to_na)
importFrom(targets,tar_assert_package)
importFrom(usethis,edit_file)

----
Work\R\ExampleData.R
#' ST000284 Dataset
#' 
#' This dataset is used as an example in the package.
#' 
#' @format A `DatasetExperiment` object
#' @name ST000284
#' @source https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&StudyID=ST000284&StudyType=MS&ResultType=1
#' @aliases ST000284
"ST000284"


----
Work\R\Plots.R
#' Plot density plots for one variable with a legend
#' 
#' This function generates density plots for one variable, comparing the original and imputed data, with a legend indicating the data source.
#' 
#' @param original_var The original variable data.
#' @param imputed_var The imputed variable data.
#' @return A ggplot object displaying the density plots.
#' @export
#' @examples
#' plot_density_single_with_legend(original_var = data$original_var, imputed_var = data$imputed_var)
plot_density_single_with_legend <- function(original_var, imputed_var) {
  original_density <- ggplot(data = NULL, aes(x = original_var, fill = "Original")) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot Comparison") +
    geom_density(data = NULL, aes(x = imputed_var, fill = "Imputed"), alpha = 0.5) +
    labs(title = "Density Plot Comparison") +
    scale_fill_manual(name = "Data", values = c("Original" = "blue", "Imputed" = "red")) +
    theme_minimal()
  
  print(original_density)
}

#' Plot boxplots for multiple columns
#' 
#' This function generates vertical boxplots for multiple columns of a dataset.
#' 
#' @param data The dataset containing the columns to be plotted.
#' @param title The title of the plot.
#' @return A ggplot object displaying the boxplots.
#' @export
#' @examples
#' plot_boxplots(data = my_data, title = "Boxplot of Columns")
plot_boxplots <- function(data, title = "Boxplot of Columns") {
  # Convert data to long format for boxplot
  data_long <- reshape2::melt(data)
  
  # Plot vertical boxplots
  ggplot(data_long, aes(x = variable, y = value)) +
    geom_boxplot() +
    labs(title = title, x = "Columns", y = "Values") +
    coord_flip() # Rotate the plot to make it vertical
}

#' Plot a heatmap of the data
#' 
#' This function generates a heatmap of the provided dataset.
#' 
#' @param dataset_experiment The dataset for which the heatmap will be generated.
#' @param na_colour The color to represent missing values in the heatmap.
#' @return A ggplot object displaying the heatmap.
#' @export
#' @examples
#' plot_heatmap(dataset_experiment = my_dataset, na_colour = "#FF00E4")
plot_heatmap <- function(dataset_experiment, na_colour = "#FF00E4") {
  # Create a heatmap of the data
  C <- DatasetExperiment_heatmap(na_colour = na_colour)
  chart_plot(C, dataset_experiment)
}

#' Generate a missing values plot
#' 
#' This function generates a missing values plot using the VIM package's aggr function and saves the plot to a specified directory with a given filename.
#' 
#' @param dataset_experiment The dataset for which the missing values plot will be generated.
#' @param out_dir The directory where the plot will be saved.
#' @param out_name The filename for the saved plot.
#'
#' @return NULL
#' @export
#' @examples
#' missing_values_plot(dataset_experiment = my_dataset, out_dir = "output", out_name = "missing_plot.png")
missing_values_plot <- function(dataset_experiment, out_dir, out_name) {
  plt <- VIM::aggr(SummarizedExperiment::assay(dataset_experiment))
  save_plot(plt, out_dir, out_name)
}


#' Generate a distribution boxplot
#' 
#' This function generates a distribution boxplot for a specified factor in the dataset.
#' 
#' @param dataset_experiment The dataset for which the boxplot will be generated.
#' @param factor_name The name of the factor variable.
#' @param per_class Logical indicating whether to generate separate boxplots for each class of the factor.
#' @return A ggplot object displaying the distribution boxplot.
#' @export
#' @examples
#' distribution_boxplot(dataset_experiment = my_dataset, factor_name = "factor", per_class = TRUE)
distribution_boxplot <- function(dataset_experiment, factor_name, per_class = FALSE) {
  C <- DatasetExperiment_dist(factor_name = factor_name, per_class = per_class)
  chart_plot(C, dataset_experiment)
}

#' Generate a before-after plot
#' 
#' This function generates a before-after plot comparing distributions before and after a certain process or treatment.
#' 
#' @param DE_before The dataset before the process or treatment.
#' @param DE_after The dataset after the process or treatment.
#' @param factor_name The name of the factor variable for stratification.
#' @return A ggplot object displaying the before-after plot.
#' @export
#' @examples
#' ba_plot(DE_before = before_data, DE_after = after_data, factor_name = "sample_type")
ba_plot <- function(DE_before, DE_after, factor_name = "sample_type") {
  C <- compare_dist(factor_name = factor_name)
  plot <- chart_plot(C, DE_before, DE_after)
  return(plot)
}

#' Generate a batch plot
#' 
#' This function generates a batch plot showing the relationship between a feature and run order, stratified by batches and quality control (QC) samples.
#' 
#' @param dataset_experiment The dataset for which the batch plot will be generated.
#' @param order_col The column representing run order.
#' @param batch_col The column representing batches.
#' @param qc_col The column representing QC samples.
#' @param qc_label The label for QC samples.
#' @param colour_by_col The column used for coloring in the plot.
#' @param feature_to_plot The feature to be plotted.
#' @param ylab The label for the y-axis.
#' @param title The title of the plot.
#' @return A ggplot object displaying the batch plot.
#' @export
#' @examples
#' batch_plot(dataset_experiment = my_dataset, order_col = "order", batch_col = "batch", qc_col = "qc", qc_label = "qc_label", colour_by_col = "colour", feature_to_plot = "feature", ylab = "Peak area", title = "Feature vs run_order")
batch_plot <- function(dataset_experiment, order_col, batch_col, qc_col, qc_label, colour_by_col, feature_to_plot, ylab = "Peak area", title = "Feature vs run_order") {
  corrected_feature_name <- make.names(feature_to_plot) # Make sure the feature name is the same as the column name in the dataset
  
  C <- feature_profile(
    run_order = order_col,
    qc_label = qc_label,
    qc_column = qc_col,
    colour_by = colour_by_col,
    feature_to_plot = corrected_feature_name,
    plot_sd = FALSE
  )
  
  chart_plot(C, dataset_experiment) + ylab(ylab) + ggtitle(title)
}

#' Generate a PCA plot
#' 
#' This function generates a PCA plot showing the principal component scores colored by a specified factor.
#' 
#' @param dataset_experiment The dataset for which the PCA plot will be generated.
#' @param factor_name The name of the factor variable used for coloring.
#' @param nPCs The number of principal components to include in the analysis.
#' @return A ggplot object displaying the PCA plot.
#' @export
#' @examples
#' plot_pca(dataset_experiment = my_dataset, factor_name = "sample_type", nPCs = 5)
plot_pca <- function(dataset_experiment, factor_name = "sample_type", nPCs = 5) {
  M <- knn_impute() + mean_centre() + PCA(number_components = nPCs)
  M <- model_apply(M, dataset_experiment)
  C <- pca_scores_plot(factor_name = factor_name, ellipse_type = "norm")
  chart_plot(C, M[3])
}

#' Generate an outliers plot
#' 
#' This function generates a plot showing outliers detected using Hotelling's T-squared statistic in PCA.
#' 
#' @param dataset_experiment The dataset for which the outliers plot will be generated.
#' @param nPCs The number of principal components to include in the analysis.
#' @param out_dir The directory where the plot will be saved.
#' @param out_name The filename for the saved plot.
#' @return NULL
#' @export
#' @examples
#' plot_outliers(dataset_experiment = my_dataset, nPCs = 5, out_dir = "output", out_name = "outliers_plot.png")
plot_outliers <- function(dataset_experiment, nPCs = 5, out_dir, out_name) {
  plt1 <- plot_hotelling_pca(dataset_experiment, nPCs = 5)
  plt2 <- plot_hotelling_obs(dataset_experiment, nPCs = 5, nPCs_to_plot = 2)
  plt <- cowplot::plot_grid(plt1, plt2, ncol = 2)
  save_plot(plt, out_dir, out_name)
}

#' Generate a PCA Hotelling's T-squared plot
#' 
#' This function generates a PCA Hotelling's T-squared plot showing the principal component scores and confidence ellipses.
#' 
#' @param dataset_experiment The dataset for which the plot will be generated.
#' @param nPCs The number of principal components to include in the analysis.
#' @return A ggplot object displaying the PCA Hotelling's T-squared plot.
#' @export
#' @examples
#' plot_hotelling_pca(dataset_experiment = my_dataset, nPCs = 5)
plot_hotelling_pca <- function(dataset_experiment, nPCs = 5) {
  # Perform PCA
  M <- structToolbox::knn_impute() + structToolbox::mean_centre() + structToolbox::PCA(number_components = nPCs)
  M <- structToolbox::model_apply(M, dataset_experiment)
  
  # Extract pca_scores
  pca_scores <- M[3]$scores$data %>% as_tibble()
  
  # Calculate Hotelling's T2 ellipse params
  res_PCs <- HotellingEllipse::ellipseParam(data = pca_scores, k = 2, pcx = 1, pcy = 2)
  # Extract Hotelling's T2 values
  T2 <- purrr::pluck(res_PCs, "Tsquare", "value")
  
  # Extract ellipse params for plotting
  a99 <- purrr::pluck(res_PCs, "Ellipse", "a.99pct")
  b99 <- purrr::pluck(res_PCs, "Ellipse", "b.99pct")
  a95 <- purrr::pluck(res_PCs, "Ellipse", "a.95pct")
  b95 <- purrr::pluck(res_PCs, "Ellipse", "b.95pct")
  
  # Plot PCA scores
  plt <- pca_scores %>%
    ggplot(aes(x = PC1, y = PC2)) +
    geom_ellipse(aes(x0 = 0, y0 = 0, a = a99, b = b99, angle = 0), linewidth = .5, linetype = "dotted", fill = "white") +
    geom_ellipse(aes(x0 = 0, y0 = 0, a = a95, b = b95, angle = 0), linewidth = .5, linetype = "dashed", fill = "white") +
    geom_point(aes(fill = T2), shape = 21, size = 3, color = "black") +
    scale_fill_viridis_c(option = "viridis") +
    geom_hline(yintercept = 0, linetype = "solid", color = "black", linewidth = .1) +
    geom_vline(xintercept = 0, linetype = "solid", color = "black", linewidth = .1) +
    labs(
      title = "Scatterplot of PCA scores", subtitle = "PC1 vs. PC2", x = "PC1", y = "PC2",
      fill = "T2", caption = "Hotelling's T2 ellipse with 99(exterior line) and 95(interior line) confidence intervals"
    ) +
    theme_bw() +
    theme(panel.grid = element_blank())
  
  return(plt)
}

#' Generate a PCA Hotelling's T-squared observations plot
#' 
#' This function generates a PCA Hotelling's T-squared observations plot showing the T-squared values for each observation.
#' 
#' @param dataset_experiment The dataset for which the plot will be generated.
#' @param nPCs The number of principal components to include in the analysis.
#' @param nPCs_to_plot The number of principal components to plot the ellipses for.
#' @return A ggplot object displaying the PCA Hotelling's T-squared observations plot.
#' @export
#' @examples
#' plot_hotelling_obs(dataset_experiment = my_dataset, nPCs = 5, nPCs_to_plot = 2)
plot_hotelling_obs <- function(dataset_experiment, nPCs = 5, nPCs_to_plot = 2) {
  # Perform PCA
  M <- structToolbox::knn_impute() + structToolbox::mean_centre() + structToolbox::PCA(number_components = nPCs)
  M <- structToolbox::model_apply(M, dataset_experiment)
  
  # Extract pca_scores
  pca_scores <- M[3]$scores$data %>% as_tibble()
  
  # Calculate Hotelling's T2 ellipse params
  res_PCs <- ellipseParam(data = pca_scores, k = nPCs_to_plot)
  
  # Plot
  plt <- tibble(
    T2 = purrr::pluck(res_PCs, "Tsquare", "value"),
    obs = rownames(pca_scores)
  ) %>%
    ggplot() +
    geom_point(aes(x = obs, y = T2, fill = T2), shape = 21, size = 3, color = "black") +
    geom_segment(aes(x = obs, y = T2, xend = obs, yend = 0), linewidth = .5) +
    scale_fill_gradient(low = "black", high = "red", guide = "none") +
    geom_hline(yintercept = pluck(res_PCs, "cutoff.99pct"), linetype = "dashed", color = "darkred", linewidth = .5) +
    geom_hline(yintercept = pluck(res_PCs, "cutoff.95pct"), linetype = "dashed", color = "darkblue", linewidth = .5) +
    annotate("text", x = 80, y = 13, label = "99% limit", color = "darkred") +
    annotate("text", x = 80, y = 9, label = "95% limit", color = "darkblue") +
    labs(
      x = "Observations", y = paste0("Hotelling’s T-squared (", nPCs_to_plot, "PCs)"),
      fill = "T2 stats", caption = "Hotelling’s T-squared vs. Observations"
    ) +
    theme_bw() +
    theme(panel.grid = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
  
  return(plt)
}

----
Work\R\createExperiment.R
#' Sort by sample_id
#' 
#' @param df A dataframe with a sample_id column.
#'
#' @return A data frame sorted by sample_id.
#' @export
#'
#' @examples
#' sort_by_sample_id(data)
sort_by_sample_id <- function(df) {
  # sort by sample_id
  df <- df[order(df$sample_id), ]
  return(df)
}


#' Process the dataset to create the DatasetExperiment object
#'
#' @param dataMatrix A matrix with samples as rows and features as columns.
#' @param sampleMetadata A data frame with the sample metadata.
#' @param variableMetadata A data frame with the variable metadata.
#' @param experiment_name The name for the experiment.
#' @param experiment_description The description for the experiment.
#'
#' @return A \code{DatasetExperiment} object.
#' @export
#'
#' @examples
#' warper_createExperiment(dataMatrix, sampleMetadata, variableMetadata, experiment_name = "Name", experiment_description = "Description")
warper_createExperiment <- function(dataMatrix, sampleMetadata, variableMetadata,
                             experiment_name = "Name", experiment_description = "Description") {
  # Convert tibbles to data frames
  dataMatrix <- as.data.frame(dataMatrix)
  sampleMetadata <- as.data.frame(sampleMetadata)
  variableMetadata <- as.data.frame(variableMetadata)
  
  # dataMatrix should have a row for each sample and a column for each feature
  # Check dimensions
  if (nrow(dataMatrix) != nrow(sampleMetadata)) {
    stop("Number of rows in dataMatrix and sampleMetadata do not match.")
  }

  if (ncol(dataMatrix) != nrow(variableMetadata)) {
    stop("Number of columns in dataMatrix and rows in variableMetadata do not match.")
  }

  # convert 0 to NA
  dataMatrix <- dataMatrix %>% mutate_if(is.character, as.numeric)
  dataMatrix[dataMatrix == 0] <- NA
  dataMatrix <- data.frame(lapply(dataMatrix, as.numeric), check.names = FALSE)
  
  
  # Drop dataMatrix$sample_id
  dataMatrix$sample_id <- NULL


  # Make metabolites underscore for compatibility
  colnames(dataMatrix) <- gsub("-", "_", colnames(dataMatrix))


  # Check row/col names match
  rownames(dataMatrix) <- sampleMetadata$sample_id
  rownames(sampleMetadata) <- sampleMetadata$sample_id
  rownames(variableMetadata) <- colnames(dataMatrix)


  # Create a DatasetExperiment object
  # require(structToolbox)
  DE <- DatasetExperiment(
    data = dataMatrix,
    sample_meta = sampleMetadata,
    variable_meta = variableMetadata,
    name = experiment_name,
    description = experiment_description
  )
  
  # Make the metabolite names correct
  DE@colData@listData$annotation <- variableMetadata$annotation

  return(DE)
}



----
Work\R\filter.R
#' Filter Missing values
#' 
#' Filter the missing values in a dataset experiment.
#' 
#' @param dataset_exp The dataset experiment object to filter missing values from.
#' @param threshold The threshold for filtering missing values (default is 0.8).
#' 
#' @return A dataset experiment object with missing values filtered out.
#' 
#' @export
#' 
#' @examples
#' filtered_data <- filter_MV(dataset_exp, threshold = 0.8)
filter_MV <- function(dataset_exp, threshold = 0.8) {
  # Check if threshold is specified and valid
  if (missing(threshold)) {
    threshold <- 0.8 # Default threshold
    cat("No threshold specified. Defaulting to 0.8.\n")
  } else {
    # Check if threshold is between 0 and 1
    if (threshold < 0 || threshold > 1) {
      stop("Threshold must be between 0 and 1.")
    }
    # Check if threshold is a numeric value
    if (!is.numeric(threshold)) {
      stop("Threshold must be a numeric value.")
    }
    cat(paste0("Threshold value: ", threshold, "\n"))
  }
  
  ncols <- ncol(SummarizedExperiment::assay(dataset_exp))
  
  M <- mv_sample_filter(mv_threshold = threshold * 100) + mv_feature_filter(threshold = threshold * 100, method = "across", factor_name = "sample_type")
  M <- model_apply(M, dataset_exp)
  
  filtered_experiment <- predicted(M)
  
  # Calculate the number of rows and columns removed
  removed_rows <- nrow(SummarizedExperiment::assay(dataset_exp)) - nrow(SummarizedExperiment::assay(filtered_experiment))
  removed_cols <- ncol(SummarizedExperiment::assay(dataset_exp)) - ncol(SummarizedExperiment::assay(filtered_experiment))
  
  # Print information about removed rows and columns
  if (removed_rows == 0) {
    cat("No rows removed\n")
  } else {
    cat(paste0("Number of rows removed: ", removed_rows, "\n"))
  }
  
  if (removed_cols == 0) {
    cat("No columns removed\n")
  } else {
    cat(paste0("Number of columns removed: ", removed_cols, "\n"))
  }
  
  return(filtered_experiment)
}

#' Make 0 as NA
#' 
#' Replace 0 values with NA in a dataset experiment.
#' 
#' @param dataset_exp The dataset experiment object.
#' 
#' @return A dataset experiment object with 0 values replaced by NA.
#' 
#' @export
#' 
#' @examples
#' modified_dataset <- zero_to_na(dataset_exp)
zero_to_na <- function(dataset_exp) {
  modified_de <- dataset_exp
  df <- SummarizedExperiment::assay(dataset_exp)
  df <- df %>% mutate_if(is.character, as.numeric)
  df[df == 0] <- NA
  SummarizedExperiment::assay(modified_de, withDimnames = FALSE) <- df
  return(modified_de)
}

#' Filter Blanks
#' 
#' Filter blanks from the dataset experiment.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param fold_change The fold change threshold for blank filtering.
#' @param blank_label The label for blanks.
#' @param qc_label The label for quality control samples.
#' @param factor_name The factor column name.
#' @param fraction_in_blank The fraction of values in blank (default is 0).
#' 
#' @return A dataset experiment object with blanks filtered out.
#' 
#' @export
#' 
#' @examples
#' filtered_data <- filter_blanks(dataset_experiment, fold_change = 20, blank_label = "blank", qc_label = "QC", factor_name = "sample_type", fraction_in_blank = 0)
filter_blanks <- function(dataset_experiment, fold_change = 20, blank_label = "blank", qc_label = "QC", factor_name = "sample_type", fraction_in_blank = 0) {
  M <- blank_filter(
    fold_change = fold_change,
    blank_label = blank_label,
    qc_label = qc_label,
    factor_name = factor_name,
    fraction_in_blank = fraction_in_blank
  )
  M <- model_apply(M, dataset_experiment)
  filtered_experiment <- predicted(M)
  
  return(filtered_experiment)
}

#' Filter Outliers
#' 
#' Filter outliers from the dataset experiment.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param nPCs The number of principal components for PCA.
#' @param conf.limit The confidence limit for outlier detection.
#' 
#' @return A dataset experiment object with outliers filtered out.
#' 
#' @export
#' 
#' @examples
#' filtered_data <- filter_outliers(dataset_experiment, nPCs = 5, conf.limit = "0.95")
filter_outliers <- function(dataset_experiment, nPCs = 5, conf.limit = c("0.95", "0.99")) {
  # Check if dataset_experiment is a dataset_experiment object
  if (!inherits(dataset_experiment, "DatasetExperiment")) {
    stop("dataset_experiment must be a DatasetExperiment object")
  }
  # Check if nPCs is numeric
  if (!is.numeric(nPCs)) {
    stop("nPCs must be a numeric value")
  }
  # Check if conf.limit is either 0.95 or 0.99
  if (!conf.limit %in% c("0.95", "0.99")) {
    stop("conf.limit must be either 0.95 or 0.99")
  }
  
  # Perform PCA
  M <- structToolbox::knn_impute() + structToolbox::mean_centre() + structToolbox::PCA(number_components = nPCs)
  M <- structToolbox::model_apply(M, dataset_experiment)
  
  # Extract pca_scores
  pca_scores <- M[3]$scores$data %>% as_tibble()
  
  # Calculate Hotelling's T2 ellipse params
  res_PCs <- HotellingEllipse::ellipseParam(data = pca_scores, k = 2, pcx = 1, pcy = 2)
  # Extract Hotelling's T2 values
  T2 <- purrr::pluck(res_PCs, "Tsquare", "value")
  
  # Extract cutoff values for Hotelling's T2
  cutoff_99 <- purrr::pluck(res_PCs, "cutoff.99pct")
  cutoff_95 <- purrr::pluck(res_PCs, "cutoff.95pct")
  
  # Select Observations that are above the 99% confidence interval
  outliers_99 <- pca_scores %>%
    mutate(obs = rownames(pca_scores)) %>%
    filter(T2 > cutoff_99)
  
  # Select Observations that are above the 99% confidence interval
  outliers_95 <- pca_scores %>%
    mutate(obs = rownames(pca_scores)) %>%
    filter(T2 > cutoff_95)
  
  
  # Remove outliers from experiment
  if (conf.limit == "0.95") {
    FT <- structToolbox::filter_by_name(mode = "exclude", dimension = "sample", outliers_95$obs)
    Filtered <- structToolbox::model_apply(FT, dataset_experiment)
  }
  if (conf.limit == "0.99") {
    FT <- structToolbox::filter_by_name(mode = "exclude", dimension = "sample", outliers_99$obs)
    Filtered <- structToolbox::model_apply(FT, dataset_experiment)
  }
  
  return(predicted(Filtered))
}

----
Work\R\imputation.R
#' Impute Missing Values
#' 
#' Impute missing values in a dataset experiment using various methods.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param method The imputation method to use.
#' @param k The parameter for some imputation methods (default: 5).
#' 
#' @return The dataset experiment object with missing values imputed.
#' 
#' @export
#' 
#' @examples
#' impute_warper(dataset_experiment, method = "mean")
impute_warper <- function(dataset_experiment, method, k = 5) {
  switch(method,
         "mean"    = impute_mean(dataset_experiment),
         "median"  = impute_median(dataset_experiment),
         'RF'      = impute_RF(dataset_experiment),
         "QRILC"   = impute_QRILC(dataset_experiment),
         "kNN"     = impute_kNN(dataset_experiment, k = k),
         "SVD"     = impute_SVD(dataset_experiment, nPCs = k),
         "bpca"    = impute_bpca(dataset_experiment, nPCs = k),
         "ppca"    = impute_ppca(dataset_experiment, nPCs = k)
  )
}

#' Impute Mean
#' 
#' Impute missing values in a dataset experiment using the mean.
#' 
#' @param dataset_experiment The dataset experiment object.
#' 
#' @return The dataset experiment object with missing values imputed using the mean.
#' 
#' @export
#' 
#' @examples
#' impute_mean(dataset_experiment)
impute_mean <- function(dataset_experiment) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Impute missing values column-wise
  data_matrix[] <- lapply(data_matrix, function(x) {
    x[is.na(x)] <- mean(x, na.rm = T)
    x
  })
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- data_matrix
  
  return(DE_imp)
}

#' Impute Median
#' 
#' Impute missing values in a dataset experiment using the median.
#' 
#' @param dataset_experiment The dataset experiment object.
#' 
#' @return The dataset experiment object with missing values imputed using the median.
#' 
#' @export
#' 
#' @examples
#' impute_median(dataset_experiment)
impute_median <- function(dataset_experiment) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Impute missing values column-wise
  data_matrix[] <- lapply(data_matrix, function(x) {
    x[is.na(x)] <- median(x, na.rm = T)
    x
  })
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- data_matrix
  
  return(DE_imp)
}

#' Impute Random Forest
#' 
#' Impute missing values in a dataset experiment using random forest.
#' 
#' @param dataset_experiment The dataset experiment object.
#' 
#' @return The dataset experiment object with missing values imputed using random forest.
#' 
#' @export
#' 
#' @examples
#' impute_RF(dataset_experiment)
impute_RF <- function(dataset_experiment) {
  # doParallel::registerDoParallel(cores=4)
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Impute missing values
  # forest_result <- missForest::missForest(SummarizedExperiment::assay(dataset_experiment), maxiter = 100, ntree = 1000, parallelize = "forests")
  forest_result <- missForest::missForest(SummarizedExperiment::assay(dataset_experiment), maxiter = 100, ntree = 1000)
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- forest_result$ximp
  
  return(DE_imp)
}

#' Impute QRILC
#' 
#' Impute missing values in a dataset experiment using QRILC.
#' 
#' @param dataset_experiment The dataset experiment object.
#' 
#' @return The dataset experiment object with missing values imputed using QRILC.
#' 
#' @export
#' 
#' @examples
#' impute_QRILC(dataset_experiment)
impute_QRILC <- function(dataset_experiment) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Impute missing values
  imputed_data <- imputeLCMD::impute.QRILC(SummarizedExperiment::assay(dataset_experiment))
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- imputed_data[[1]]
  
  return(DE_imp)
}


#' Impute kNN
#' 
#' Impute missing values in a dataset experiment using kNN.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param k The number of neighbors for kNN imputation.
#' 
#' @return The dataset experiment object with missing values imputed using kNN.
#' 
#' @export
#' 
#' @examples
#' impute_kNN(dataset_experiment, k = 5)
impute_kNN <- function(dataset_experiment, k = k) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Impute missing values
  imputed_data <- impute::impute.knn(as.matrix(data_matrix), K = k)
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- imputed_data$data
  
  return(DE_imp)
}



#' Impute SVD
#' 
#' Impute missing values in a dataset experiment using SVD.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param nPCs The number of principal components for SVD.
#' 
#' @return The dataset experiment object with missing values imputed using SVD.
#' 
#' @export
#' 
#' @examples
#' impute_SVD(dataset_experiment, nPCs = 5)
impute_SVD <- function(dataset_experiment, nPCs = k, center = TRUE, ...) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Call wrapper function to impute missing values
  impute_results <- pcaMethods::pca(data_matrix, method = "svdImpute", nPcs = nPCs, center = center, ...)
  plotPcs(impute_results, type = "scores")
  imputed_data <- pcaMethods::completeObs(impute_results)
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- imputed_data
  
  return(DE_imp)
}

#' Impute BPCA
#' 
#' Impute missing values in a dataset experiment using BPCA.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param nPCs The number of principal components for BPCA.
#' 
#' @return The dataset experiment object with missing values imputed using BPCA.
#' 
#' @export
#' 
#' @examples
#' impute_bpca(dataset_experiment, nPCs = 5)
impute_bpca <- function(dataset_experiment, nPCs = k, ...) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Call wrapper function to impute missing values
  pc <- pcaMethods::pca(data_matrix, method = "bpca", nPCs = nPCs, ...)
  
  imputed_data <- pcaMethods::completeObs(pc)
  
  slplot(pc)
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- imputed_data
  
  return(DE_imp)
}



#' Impute PPCA
#' 
#' Impute missing values in a dataset experiment using PPCA.
#' 
#' @param dataset_experiment The dataset experiment object.
#' @param nPCs The number of principal components for PPCA.
#' 
#' @return The dataset experiment object with missing values imputed using PPCA.
#' 
#' @export
#' 
#' @examples
#' impute_ppca(dataset_experiment, nPCs = 5)
impute_ppca <- function(dataset_experiment, nPCs = k, ...) {
  # Create a copy of the DatasetExperiment object
  DE_imp <- dataset_experiment
  
  # Extract the data matrix
  data_matrix <- SummarizedExperiment::assay(dataset_experiment)
  
  # Call wrapper function to impute missing values
  impute_results <- pcaMethods::pca(data_matrix, method = "ppca", nPcs = nPCs, ...)
  pcaMethods::plotPcs(impute_results, type = "scores")
  imputed_data <- pcaMethods::completeObs(impute_results)
  
  # Replace missing values with imputed values
  SummarizedExperiment::assay(DE_imp, withDimnames = FALSE) <- imputed_data
  
  return(DE_imp)
}

----
Work\R\normalization.R
##### Probabilistic Quotient normalization (PQN) #####
#' Perform Probabilistic Quotient normalization (PQN)
#' 
#' @param dataset_experiment A `DatasetExperiment` object
#' @param qc_label Label for quality control samples
#' @param factor_name Name of the factor to use for normalization
#' 
#' @return Normalized A `DatasetExperiment` object
#' @export
#' 
#' @examples
#' normalize_pqn(dataset_experiment, qc_label, factor_name)
normalize_pqn <- function(dataset_experiment, qc_label, factor_name) {
  # It is not recommended to use pqn when the number of columns is greater than the number of rows
  if (ncol(dataset_experiment) > nrow(dataset_experiment)) {
    warning("The number of columns is greater than the number of rows. It is not recommended to use pqn when the number of Metabolites is greater than the number of Samples")
  }
  M <- pqn_norm(qc_label = qc_label, factor_name = factor_name)
  
  M <- model_apply(M, dataset_experiment)
  
  return(predicted(M))
}


##### Vector Length Normalization (VLN) #####
#' Perform Vector Length Normalization (VLN)
#' 
#' @param dataset_experiment A `DatasetExperiment` object
#' 
#' @return Normalized A `DatasetExperiment` object
#' @export
#' 
#' @examples
#' normalize_vln(dataset_experiment)
normalize_vln <- function(dataset_experiment) {
  M <- vec_norm()
  
  M <- model_apply(M, dataset_experiment)
  
  return(predicted(M))
}

#### Constant Sum Normalization (CSN) #### (Maybe should be discontinued)
#' Perform Constant Sum Normalization (CSN)
#' 
#' @param dataset_experiment A `DatasetExperiment` object
#' @param scaling_factor Scaling factor for normalization
#' 
#' @return Normalized A `DatasetExperiment` object
#' @export
#' 
#' @examples
#' normalize_csn(dataset_experiment, scaling_factor = 1)
normalize_csn <- function(dataset_experiment, scaling_factor = 1) {
  M <- constant_sum_norm(scaling_factor = scaling_factor)
  
  M <- model_apply(M, dataset_experiment)
  
  return(predicted(M))
}


##### Normalization with MetaboAnalystR
#' Normalize A `DatasetExperiment` object using MetaboAnalystR
#' 
#' @param dataset_experiment A `DatasetExperiment` object
#' @param factor_col Column containing factor information for normalization
#' @param sample_id_col Column containing sample IDs
#' @param rowNorm Type of row normalization (options: "QuantileNorm", "CompNorm", "SumNorm", "MedianNorm", "SpecNorm", or "NULL")
#' @param transNorm Type of transformation normalization (options: "LogNorm", "CrNorm", or "NULL")
#' @param scaleNorm Type of scaling normalization (options: "MeanCenter", "AutoNorm", "ParetoNorm", "RangeNorm", or "NULL")
#' @param ref Reference feature for 'CompNorm' normalization
#' @param ratio Boolean indicating whether to apply ratio normalization
#' @param ratioNum Number of samples for ratio normalization
#' @param out_dir Output directory for saving files
#'
#' @return Normalized A `DatasetExperiment` object
#' @export
#' 
#' @examples
#' normalize_metab(dataset_experiment, factor_col, sample_id_col, rowNorm = NULL, transNorm = NULL, scaleNorm = NULL, ref = NULL, ratio = FALSE, ratioNum = 20, out_dir)
normalize_metab <- function(dataset_experiment, factor_col, sample_id_col, rowNorm = "NULL", transNorm = "NULL", scaleNorm = "NULL", ref = NULL, ratio = FALSE, ratioNum = 20, out_dir) {
  # Check if the rowNorm argument is valid
  if (!is.null(rowNorm) && !rowNorm %in% c("QuantileNorm", "CompNorm", "SumNorm", "MedianNorm", "SpecNorm", "NULL")) {
    stop("Invalid rowNorm argument. Must be one of 'QuantileNorm', 'CompNorm', 'SumNorm', 'MedianNorm', 'SpecNorm', or NULL.")
  }
  # Check if the transNorm argument is valid
  if (!is.null(transNorm) && !transNorm %in% c("LogNorm", "CrNorm", "NULL")) {
    stop("Invalid transNorm argument. Must be one of 'LogNorm', 'CrNorm', or 'NULL'.")
  }
  # Check if the scaleNorm argument is valid
  if (!is.null(scaleNorm) && !scaleNorm %in% c("MeanCenter", "AutoNorm", "ParetoNorm", "RangeNorm", "NULL")) {
    stop("Invalid scaleNorm argument. Must be one of 'MeanCenter', 'AutoNorm', 'ParetoNorm', 'RangeNorm', or 'NULL'.")
  }
  if (rowNorm == "CompNorm" & is.null(ref)) {
    stop("Reference Feature must be specified for 'CompNorm' normalization.")
  }
  
  ref <- make.names(ref)
  
  withr::with_dir(out_dir, {
    dir.create("TempData", showWarnings = FALSE)
    
    
    # Create a metaboanalyst object
    toMetaboAnalyst(dataset_experiment, factor_col, sample_id_col)
    mSet <- MetaboAnalyst_load_data()
    mSet <- metaboNorm(mSet, rowNorm, transNorm, scaleNorm, ref, ratio, ratioNum, out_dir)
    save_metabo(mSet)
    
    # Now we have to rebuild the dataset_experiment object
    normalizedData <- read.csv("TempData/data_normalized.csv", header = F, row.names = 1) %>%
      t() %>%
      as.data.frame() %>%
      rename(sample_id = V1) %>%
      arrange(sample_id)
    
    rownames(normalizedData) <- normalizedData$sample_id
    
    if (rowNorm == "CompNorm" & !is.null(ref)) {
      Filt <- structToolbox::filter_by_name(
        mode = "exclude",
        dimension = "variable",
        ref
      )
      Filt <- model_apply(Filt, dataset_experiment)
      dataset_experiment <- predicted(Filt)
    }
    
    
    # Reorder the rows of df1 based on the row names from df2
    order <- assay(dataset_experiment) %>% rownames()
    sortedData <- normalizedData[order, , drop = FALSE]
    sortedData <- sortedData %>% select(-sample_id, -Label)
    # Modify the dataset_experiment object
    dataset_experiment <- data.modify(dataset_experiment, sortedData)
  })
  return(dataset_experiment)
}

----
Work\R\target_factories.R
#' Load data from files into data frames
#' 
#' @param output_name The name of the output target.
#' @param dataMatrixFile Path to the data matrix file.
#' @param sampleMetadataFile Path to the sample metadata file.
#' @param variableMetadataFile Path to the variable metadata file (optional).
#' @param dataSep The separator used in the dataMatrixfile (default is ",").
#' @param sampleSep The separator used in the sampleMetadataFile (default is ",").
#' @param variableSep The separator used in the variableMetadataFile (default is ",").
#' 
#' @return A list of targets to load and read data matrix and sample metadata.
#' 
#' @export
#' 
#' @examples
#' load_data(data_loaded, "Data/data.csv", "Data/metadata.csv", "Data/variable_metadata.csv", separator = ",")
load_data <- function(output_name, dataMatrixFile, sampleMetadataFile, variableMetadataFile = NULL, dataSep = ",", sampleSep = ",", variableSep = ",") {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  name_data <- paste0(target_name, "_data") # Generate the name for the data target
  name_sample <- paste0(target_name, "_sample") # Generate the name for the sample target
  name_variable <- paste0(target_name, "_variable") # Generate the name for the variable target
  name_head <- paste0(target_name, "_headDataMatrix")
  name_matrixFile <- paste0(target_name, "_matrixFile")
  name_sampleFile <- paste0(target_name, "_sampleFile")
  name_variableFile <- paste0(target_name, "_variableFile")
  read_matrix_command <- substitute(read.csv(name_matrixFile, sep = dataSep, header = TRUE), env = list(name_matrixFile = as.name(name_matrixFile), dataSep = dataSep))
  read_sample_command <- substitute(read.csv(name_sampleFile, sep = dataSep, header = TRUE), env = list(name_sampleFile = as.name(name_sampleFile), dataSep = sampleSep))
  read_variable_command <- substitute(read.csv(name_variableFile, sep = dataSep, header = TRUE), env = list(name_variableFile = as.name(name_variableFile), dataSep = variableSep))
  read_head_command <- substitute(read.csv(name_matrixFile, sep = dataSep, header = FALSE), env = list(name_matrixFile = as.name(name_matrixFile), dataSep = dataSep))
  extract_names_command <- substitute(extract_names(name_head), env = list(name_head = as.name(name_head)))
  
  # Define targets to load and read data matrix and sample metadata
  list(
    # Target to load data matrix file
    tar_target_raw(name_matrixFile, dataMatrixFile, format = "file", deployment = "main"),
    # Target to read data matrix
    tar_target_raw(name_data, read_matrix_command, format = "fst_tbl", deployment = "main"),
    # Target to load sample metadata file
    tar_target_raw(name_sampleFile, sampleMetadataFile, format = "file", deployment = "main"),
    # Target to read sample metadata
    tar_target_raw(name_sample, read_sample_command, format = "fst_tbl", deployment = "main"),
    # If variable metadata file is provided
    if (!is.null(variableMetadataFile)) {
      list(
        # Target to load variable metadata file
        tar_target_raw(name_variableFile, variableMetadataFile, format = "file", deployment = "main"),
        # Target to read variable metadata
        tar_target_raw(name_variable, read_variable_command, format = "fst_tbl", deployment = "main")
      )
    } else { # If variable metadata file is not provided
      list(
        # Target to read data matrix without header
        tar_target_raw(name_head, read_head_command, format = "fst_tbl", deployment = "main"),
        # Target to create variable metadata from data matrix
        tar_target_raw(name_variable, extract_names_command, format = "fst_tbl", deployment = "main")
      )
    }
  )
}

#' Create DatasetExperiment object
#' 
#' @param output_name The name of the output target.
#' @param data The target name of the data with the data as data frames.
#' @param experiment_name The name of the experiment (default is "Name").
#' @param experiment_description The description of the experiment (default is "Description").
#' 
#' @return A target to create a DatasetExperiment object.
#' 
#' @export
#' 
#' @examples
#' createExperiment(experiment, data, experiment_name = "Metabolomic Assay for nutrition", experiment_description = "This is an example description")
#' @seealso [warper_createExperiment()]
createExperiment <- function(output_name, data,
                             experiment_name = "Name", experiment_description = "Description") {
  name_data <- deparse(substitute(data)) # Get the name of the input data
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  data_matrix <- paste0(name_data, "_data") # Generate the name for the data target
  data_sample <- paste0(name_data, "_sample") # Generate the name for the sample target
  data_variable <- paste0(name_data, "_variable") # Generate the name for the variable target
  # Define target to create DatasetExperiment object
  command_experiment <- substitute(warper_createExperiment(data_matrix, data_sample, data_variable, experiment_name, experiment_description), env = list(data_matrix = as.name(data_matrix), data_sample = as.name(data_sample), data_variable = as.name(data_variable), experiment_name = experiment_name, experiment_description = experiment_description))
  list(
    tar_target_raw(target_name, command_experiment, format = "qs", deployment = "main")
  )
}

#' Factorize columns in sample metadata
#' 
#' @param output_name The name of the output target.
#' @param input_name The name of the input data.
#' @param cols The columns to factorize.
#' 
#' @return A target to factorize columns in sample metadata.
#' 
#' @export
#' 
#' @examples
#' factorize_cols(factorized_experiment, input_name = experiment, cols = c("Col1", "Col2"))
#' @seealso [warper_factor_sample_col()]
factorize_cols <- function(output_name, input_name, cols) {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  data <- deparse(substitute(input_name)) # Get the name of the input data
  # Define target to factorize columns
  command_factor <- substitute(
    warper_factor_sample_col(data, col),
    env =
      list(data = as.name(data), col = cols)
  )
  list(
    tar_target_raw(target_name, command_factor, format = "qs", deployment = "main")
  )
}

#' Filter data by missing value threshold
#' 
#' @param output_name The name of the output target.
#' @param input_name The name of the input data.
#' @param threshold The threshold for missing values.
#' @param filter_outliers Logical indicating whether to filter outliers (default is TRUE).
#' @param conf.limit Confidence limit for outlier detection (default is "0.95").
#' @param out_dir The directory to save plots (optional).
#' 
#' @return A list of targets to filter data by missing value threshold.
#' 
#' @export
#' 
#' @examples
#' filter_step(filtered_experiment, input_experiment, threshold = 0.2, filter_outliers = TRUE, conf.limit = "0.95", out_dir = "Plots")
#' @seealso [filter_MV(), filter_outliers(), zero_to_na(), missing_values_plot(), plot_outliers()]
filter_step <- function(output_name, input_name, threshold, filter_outliers = TRUE, conf.limit = "0.95", out_dir) {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  input_target <- deparse(substitute(input_name)) # Get the name of the input data
  name_mv <- paste0(target_name, "_missingval") # Generate the name for the filtered data target
  name_na <- paste0(input_target, "_na") # Generate the name for the target with NAs replaced by zeros
  plot_before_name <- paste0("before_", target_name, "_plot") # Generate the name for the plot target before filtering
  plot_after_name <- paste0("after_", target_name, "_plot") # Generate the name for the plot target after filtering
  plot_outliers_name <- paste0(target_name, "_outliers_plot") # Generate the name for the outliers plot target
  
  # Define targets to save plots
  command_plot_mv_before <- substitute(missing_values_plot(dataset_experiment, out_dir, name), 
                                       env = list(dataset_experiment = as.name(input_target), 
                                                  out_dir=out_dir, name = "before_filter_mv"))
  
  command_plot_mv_after <- substitute(missing_values_plot(dataset_experiment, out_dir, name), 
                                      env = list(dataset_experiment = as.name(target_name), 
                                                 out_dir=out_dir, name = "after_filter_mv"))
  
  command_plot_outliers1 <- substitute(plot_outliers(dataset_experiment, nPCs = 5, out_dir, name),
                                       env = list(dataset_experiment = as.name(name_mv),
                                                  out_dir=out_dir, name = "outliers"))
  command_plot_outliers2 <- substitute(plot_outliers(dataset_experiment, nPCs = 5, out_dir, name),
                                       env = list(dataset_experiment = as.name(target_name),
                                                  out_dir=out_dir, name = "outliers"))
  
  command_na <- substitute(zero_to_na(dataset_experiment), 
                           env = list(dataset_experiment = as.name(input_target)))
  
  command_filter_mv <- substitute(filter_MV(dataset_experiment, threshold = threshold), 
                                  env = list(dataset_experiment = as.name(name_na), threshold = threshold))
  
  command_filter_outl <- substitute(filter_outliers(dataset_experiment, conf.limit = conf.limit), 
                                    env = list(dataset_experiment = as.name(name_mv), conf.limit = conf.limit, threshold = threshold
                                    ))
  
  
  
  # Define list of targets
  list(
    tar_target_raw(name_na, command_na, format = "qs", deployment = "main"),
    if(filter_outliers==TRUE) {
      list(
        tar_target_raw(name_mv, command_filter_mv, format = "qs", deployment = "main"),
        tar_target_raw(target_name, command_filter_outl, format = "qs", deployment = "main"),
        tar_target_raw(plot_outliers_name, command_plot_outliers1, format = "qs", deployment = "main", 
                       cue = tar_cue(mode = "always"))
        
      )
    } else {
      list(
        tar_target_raw(target_name, command_filter_mv, format = "qs", deployment = "main"),
        tar_target_raw(plot_outliers_name, command_plot_outliers2, format = "qs", deployment = "main", 
                       cue = tar_cue(mode = "always"))
      )
    },
    tar_target_raw(plot_before_name, command_plot_mv_before, format = "qs", deployment = "main", cue = tar_cue(mode = "always")),
    tar_target_raw(plot_after_name, command_plot_mv_after, format = "qs", deployment = "main", cue = tar_cue(mode = "always"))
  )
}

#' Batch correction
#' 
#' @param output_name The name of the output target.
#' @param input_name The name of the input data.
#' @param order_col The order column.
#' @param batch_col The batch column.
#' @param qc_col The QC column.
#' @param qc_label The QC label.
#' 
#' @return A target to perform batch correction.
#' 
#' @export
#' 
#' @examples
#' batch_correct(BatchCorrected_experiment, input_experiment, order_col = "Order", batch_col = "Batch", qc_col = "SampleType", qc_label = "QC")
#' @seealso [warper_batch_correction()]
batch_correct <- function(output_name, input_name, order_col, batch_col, qc_col, qc_label) {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  data <- deparse(substitute(input_name)) # Get the name of the input data
  # Define target to perform batch correction
  command_correct <- substitute(
    warper_batch_correction(data, order_col, batch_col, qc_col, qc_label),
    env =
      list(data = as.name(data), order_col = order_col, batch_col = batch_col, qc_col = qc_col, qc_label = qc_label)
  )
  list(
    tar_target_raw(target_name, command_correct, format = "qs", deployment = "main")
  )
}

#' Impute missing values
#' 
#' @param output_name The name of the output target.
#' @param input_name The name of the input data.
#' @param method The imputation method.
#' @param k The number of neighbors for KNN imputation (default is 5).
#' 
#' @return A target to impute missing values.
#' 
#' @export
#' 
#' @examples
#' impute(imputed_experiment, input_experiment, method = "knn", k = 3)
#' @seealso [impute_warper()]
impute <- function(output_name, input_name, method, k = 5) {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  data <- deparse(substitute(input_name)) # Get the name of the input data
  # Define target to impute missing values
  command <- substitute(impute_warper(data, method = method, k = k),
                        env =
                          list(data = as.name(data), method = method, k = k)
  )
  list(
    tar_target_raw(target_name, command, format = "qs", deployment = "main")
  )
}

#' Normalize data
#' 
#' @param output_name The name of the output target.
#' @param input_name The name of the input data.
#' @param factor_col The factor column.
#' @param sample_id_col The sample ID column.
#' @param rowNorm The row normalization method (optional). One of: `"QuantileNorm"`, `"CompNorm"`, `"SumNorm"`, `"MedianNorm"`, `"SpecNorm"`, or `NULL`.
#' @param transNorm The transformation normalization method (optional). One of: `"LogNorm"`, `"CrNorm"`, or `NULL`.
#' @param scaleNorm The scaling normalization method (optional). One of: `"MeanCenter"`, `"AutoNorm"`, `"ParetoNorm"`, `"RangeNorm"`, or `NULL`.
#' @param ref The reference group for normalization (optional).
#' @param out_dir The directory to save plots (optional).
#' 
#' @return A list of targets to normalize data.
#' 
#' @export
#' 
#' @examples
#' normalize(normalized_data, input_data, factor_col = "Group", sample_id_col = "Sample", rowNorm = "CompNorm", transNorm = "LogNorm", scaleNorm = "ParetoNorm", ref = "Creatinine (114.1 / 44.0)", out_dir = "Plots")
#' @seealso [normalize_metab()]
normalize <- function(output_name, input_name, factor_col, sample_id_col, rowNorm = "NULL", transNorm = "NULL", scaleNorm = "NULL", ref = NULL, out_dir) {
  target_name <- deparse(substitute(output_name)) # Get the name of the output target
  data <- deparse(substitute(input_name)) # Get the name of the input data
  clean_name <- paste0(target_name, "_cleaning_step")
  # Define target to normalize data
  command <- substitute(normalize_metab(data, factor_col, sample_id_col = sample_id_col,
                                        rowNorm, transNorm, scaleNorm, ref = ref, out_dir = out_dir),
                        env =
                          list(data = as.name(data), factor_col = factor_col, sample_id_col = sample_id_col, 
                               rowNorm = rowNorm, transNorm = transNorm, scaleNorm = scaleNorm, ref = ref, out_dir = out_dir)
  )
  list(
    tar_target_raw(target_name, command, format = "qs", deployment = "main")
    
    # tar_target_raw(clean_name, substitute(withr::with_dir(out_dir, unlink("TempData",recursive=TRUE)), 
    #                                                  env = list(out_dir = out_dir)), 
    #                format = "qs", deployment = "main", cue = tar_cue(mode = "always"), deps = target_name)
  )
}


#' Export Data
#'
#' Exports a dataset to a specified directory with a given name.
#'
#' @param output_name The name of the exported dataset.
#' @param input_name The name of the input dataset to be exported.
#' @param out_dir The directory where the exported dataset will be saved.
#'
#' @return A list containing the target for exporting the dataset.
#' @export
#'
#' @examples
#' exportData("exported_dataset.csv", my_dataset, "output_directory/")
exportData <- function(output_name, input_name, out_dir, out_name = "Processed") {
  target_name <- deparse(substitute(output_name))
  data <- deparse(substitute(input_name))
  command <- substitute(export_data(dataset_exp=data, out_dir=out_dir, out_name = out_name), env = list(data = as.name(data), out_dir = out_dir, out_name = out_name))
  list(
    tar_target_raw(target_name, command, format = "qs", deployment = "main", cue = tar_cue(mode = "always"))
  )
}


----
Work\R\utils.R
#' Signal drift and batch correction function
#' 
#' This function performs signal drift and batch correction on the given A `DatasetExperiment` object QC-RSC method
#' 
#' @param dataset_exp A `DatasetExperiment` object with samples and variables
#' @param order_col Column indicating the order of samples
#' @param batch_col Column indicating batch information
#' @param qc_col Column indicating quality control information
#' @param qc_label Label for quality control
#' 
#' @return Corrected A `DatasetExperiment` objectExperiment object
#' @export
#' 
#' @examples
#' warper_batch_correction(dataset_exp, order_col, batch_col, qc_col, qc_label)
warper_batch_correction <- function(dataset_exp, order_col, batch_col, qc_col, qc_label) {
  # Perform signal drift and batch correction using the sb_corr function
  M <- sb_corr(
    order_col = order_col,
    batch_col = batch_col,
    qc_col = qc_col,
    qc_label = qc_label,
    use_log = TRUE, # Use logarithm for transformation
    spar_lim = c(-1.5, 1.5), # Limit for signal drift correction
    min_qc = 4 # Minimum number of quality controls
  )
  
  # Apply the correction model to the A `DatasetExperiment` object
  M <- model_apply(M, dataset_exp)
  
  # Return the corrected A `DatasetExperiment` object
  return(predicted(M))
}

#' Function to convert sample columns to factors
#' 
#' This function converts specified columns in the sample metadata to factors
#' 
#' @param dataset_exp A `DatasetExperiment` object with sample metadata
#' @param col Column(s) to be converted to factors
#' 
#' @return A DatasetExperiment with specified columns converted to factors
#' @export
#' 
#' @examples
#' warper_factor_sample_col(dataset_exp, col)
warper_factor_sample_col <- function(dataset_exp, col) {
  # Convert specified columns to factors using lapply
  mod_dataset <- dataset_exp$sample_meta
  mod_dataset[, col] <- lapply(mod_dataset[, col], factor)
  
  dataset_exp$sample_meta <- mod_dataset
  
  return(dataset_exp)
}

#' Function to extract data matrix from A `DatasetExperiment` object
#' 
#' This function extracts the data matrix from a SummarizedExperiment object
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' 
#' @return Data matrix
#' @export
#' 
#' @examples
#' data.extract(dataset_exp)
data.extract <- function(dataset_exp) {
  return(SummarizedExperiment::assay(dataset_exp))
}

#' Function to modify data matrix of A `DatasetExperiment` object
#' 
#' This function replaces the data matrix in a SummarizedExperiment object with new data
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' @param data New data matrix
#' 
#' @return A `DatasetExperiment` object with modified data matrix
#' @export
#' 
#' @examples
#' data.modify(dataset_exp, data)
data.modify <- function(dataset_exp, data) {
  # Replace data matrix in the A `DatasetExperiment` object
  SummarizedExperiment::assay(dataset_exp, withDimnames = FALSE) <- data
  return(dataset_exp)
}

#' Function to extract sample metadata from A `DatasetExperiment` object
#' 
#' This function extracts the sample metadata from a SummarizedExperiment object
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' 
#' @return Sample metadata dataframe
#' @export
#' 
#' @examples
#' sample.data.extract(dataset_exp)
sample.data.extract <- function(dataset_exp) {
  return(dataset_exp$sample_meta)
}

#' Function to modify sample metadata of A `DatasetExperiment` object
#' 
#' This function replaces the sample metadata in a SummarizedExperiment object with new metadata
#' 
#' @param dataset_exp DatasetExperiment object
#' @param sample_meta New sample metadata dataframe
#' 
#' @return A `DatasetExperiment` object with modified sample metadata
#' @export
#' 
#' @examples
#' sample.data.modify(dataset_exp, sample_meta)
sample.data.modify <- function(dataset_exp, sample_meta) {
  # Replace sample metadata in the experiment dataframe
  dataset_exp$sample_meta <- sample_meta
  return(dataset_exp)
}

#' Function to extract variable metadata from A `DatasetExperiment` object
#' 
#' This function extracts the variable metadata from a SummarizedExperiment object
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' 
#' @return Variable metadata dataframe
#' @export
#' 
#' @examples
#' variable.data.extract(dataset_exp)
variable.data.extract <- function(dataset_exp) {
  return(dataset_exp$variable_meta)
}

#' Function to modify variable metadata of A `DatasetExperiment` object
#' 
#' This function replaces the variable metadata in a SummarizedExperiment object with new metadata
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' @param variable_meta New variable metadata
#' 
#' @return A `DatasetExperiment` object with modified variable metadata
#' @export
#' 
#' @examples
#' variable.data.modify(dataset_exp, variable_meta)
variable.data.modify <- function(dataset_exp, variable_meta) {
  # Replace variable metadata in the A `DatasetExperiment` object
  dataset_exp$variable_meta <- variable_meta
  return(dataset_exp)
}

#' Function to create a dataSet for Metaboanalyst
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' @param class_col Column to be used as class
#' @param sample_id Column to be used as sample ID
#' 
#' @return Nothing
#' @export
#' 
#' @examples
#' toMetaboAnalyst(dataset_exp, class_col = "sample_type", sample_id = "sample_id")
toMetaboAnalyst <- function(dataset_exp, class_col = "sample_type", sample_id = "sample_id") {
  # Extract data matrix
  dataMatrix_extracted <- SummarizedExperiment::assay(dataset_exp)
  sampleMetadata_extracted <- sample.data.extract(dataset_exp)
  
  # Extract relevant information using dplyr
  samples_name <- dplyr::pull(sampleMetadata_extracted, {{ sample_id }})
  classes <- dplyr::pull(sampleMetadata_extracted, {{ class_col }})
  
  # Create data frame for MetaboAnalyst
  MetaboDataMatrix <- data.frame(
    Sample = samples_name,
    Label = classes,
    dataMatrix_extracted
  )
  
  # Save the data frame as a CSV file
  write.csv(MetaboDataMatrix, file = "TempData/MetaboAnalystData.csv", row.names = FALSE)
  MetaboAnalyst_load_data()
}

#' Function to load the previously saved `MetaboAnalystData.csv` data into MetaboAnalyst from the TempData directory
#' 
#' @return MetaboAnalyst data object (mSet)
#' @export
#' 
#' @examples
#' MetaboAnalyst_load_data()
MetaboAnalyst_load_data <- function() {
  library(MetaboAnalystR)
  withr::with_dir("TempData", {
    # Initialize MetaboAnalyst data objects
    mSet <- InitDataObjects("conc", "stat", FALSE)
    
    # Read text data
    mSet <- Read.TextData(mSet, "MetaboAnalystData.csv", "rowu", "disc")
    
    # Print read message
    print(mSet$msgSet$read.msg)
    # Perform sanity check on data
    tryCatch(
      {
        mSet <- SanityCheckData(mSet)
      },
      error = function(e) {
        cat("Error occurred during data processing:", conditionMessage(e), "\n")
      }
    )
    
    #Reset working directory
  })
  return(mSet)
}

#' Function to normalize MetaboAnalyst data. This function performs row-wise normalization, transformation, and scaling of the metabolomic data.
#' 
#' @param mSet The MetaboAnalyst data object
#' @param rowNorm The row normalization method
#' @param transNorm The transformation normalization method
#' @param scaleNorm The scaling normalization method
#' @param ref Input the name of the reference sample or the reference feature, use " " around the name.
#' @param ratio This option is only for biomarker analysis.
#' @param ratioNum Relevant only for biomarker analysis.
#' @param out_dir The output directory for the plots
#' 
#' @return The normalized MetaboAnalyst data object
#' @export
#' 
#' @examples
#' metaboNorm(mSet, rowNorm = "NULL", transNorm = "NULL", scaleNorm = "NULL", ref = NULL, ratio = FALSE, ratioNum = 20, out_dir)
metaboNorm <- function(mSet, rowNorm = "NULL", transNorm = "NULL", scaleNorm = "NULL", ref = NULL, ratio = FALSE, ratioNum = 20, out_dir) {
  withr::with_dir("TempData", {
    # file.copy("data_orig.qs", "data_proc.qs", overwrite = TRUE)
    mSet <- ReplaceMin(mSet)
    # Perform data normalization
    mSet <- PreparePrenormData(mSet)
    mSet <- Normalization(mSet, rowNorm, transNorm, scaleNorm, ref, ratio, ratioNum)
    plot_name <- ""
    if(rowNorm != "NULL"){
      plot_name <- paste0(plot_name, "_", rowNorm, "normalization")
    }
    if(transNorm  != "NULL"){
      plot_name <- paste0(plot_name, "_", transNorm, "transformation")
    }
    if(scaleNorm != "NULL"){
      plot_name <- paste0(plot_name, "_", scaleNorm, "scaling")
    }
    # Save plots
    # View feature normalization
    tryCatch(
      {
        dir.create("Plots", showWarnings = FALSE)
        mSet <- PlotNormSummary(mSet, paste0(out_dir, "/Plots/", plot_name, "_features"), format = "png", dpi = 300, width = NA)
        
        # View sample normalization
        mSet <- PlotSampleNormSummary(mSet, paste0(out_dir, "/Plots/", plot_name, "_samples"), format = "png", dpi = 300, width = NA)
      },
      error = function(e) {
        cat("Error occurred during plot:", conditionMessage(e), "\n")
      }
    )
  })
  
  return(mSet)
}

#' Function to export MetaboAnalyst data
#' 
#' @param mSet The MetaboAnalyst data object
#' 
#' @return Nothing
#' @export
#' 
#' @examples
#' save_metabo(mSet)
save_metabo <- function(mSet) {
  withr::with_dir("TempData", {
    SaveTransformedData(mSet)
  })
}

#' Function to save plots
#'
#' @param plt The plot object
#' @param output_dir The output directory
#' @param output_name The output name for the plot file
#' 
#' @return Nothing
#' @export
#' 
#' @examples
#' save_plot(plt, output_dir, output_name)
save_plot <- function(plt, output_dir, output_name) {
  withr::with_dir(output_dir, {
    dir.create("Plots", showWarnings = FALSE)
    withr::with_dir("Plots", {
      # Save PNG
      png_name <- paste0(output_name, ".png")
      png(filename = png_name, width = 1080, height = 450, res = 100)
      plot(plt)
      dev.off()
      
      # Save SVG
      svg_name <- paste0(output_name, ".svg")
      svg(filename = svg_name, width = 12)
      plot(plt)
      dev.off()
    })
  })
}

#' Function to export data
#' 
#' @param dataset_exp A `DatasetExperiment` object
#' @param out_dir The output directory
#' @param out_name The output name of the files
#' 
#' @return Nothing
#' @export
#' 
#' @examples
#' export_data(dataset_exp, out_dir, out_name)
export_data <- function(dataset_exp, out_dir, out_name) {
  dir.create(out_dir, showWarnings = FALSE)
  withr::with_dir(out_dir, {
    # Extract data matrix
    dataMatrix <- data.extract(dataset_exp)
    # Extract sample metadata
    sampleMetadata <- sample.data.extract(dataset_exp)
    # Extract variable metadata
    variableMetadata <- variable.data.extract(dataset_exp)
    
    # Write data matrix to CSV
    write.csv(dataMatrix, file = paste0(out_name, "_data.csv"), row.names = TRUE)
    # Write sample metadata to CSV
    write.csv(sampleMetadata, file = paste0(out_name, "_sample_metadata.csv"), row.names = TRUE)
    # Write variable metadata to CSV
    write.csv(variableMetadata, file = paste0(out_name, "_variable_metadata.csv"), row.names = TRUE)
  })
}

#' Function to extract names
#' 
#' @param data 
#' 
#' @return The variableMetadata dataset for the DatasetExperiment object
#' @export
#' 
#' @examples
#' extract_names(data)
extract_names <- function(data) {
  suppressMessages(variableData <- t(data) %>% dplyr::as_tibble(.name_repair = "unique") %>%  dplyr::select(...1) %>% dplyr::rename("annotation" = ...1) %>% as.data.frame())
  variableData$annotation <- as.character(variableData$annotation) 
  return(variableData)
} 


#' Create Pipeline Function
#'
#' This function generates the code for a targets pipeline in an _targets.R file and saves it to the specified directory.
#'
#' @return Nothing is returned. The function creates an _targets.R file in the specified directory.
#' @export
#'
#' @examples
#' create_pipeline()
#'
#' @importFrom targets tar_assert_package
#' @importFrom usethis edit_file
create_pipeline <- function() {
  # Generate the code for the _targets.R file
  code <- "
  library(targets)
  library(tarchetypes)
  library(metaboPipe)

  tar_option_set(
    packages = c(
      'structToolbox', 'SummarizedExperiment', 'VIM', 'impute', 'imputeLCMD',
      'missForest', 'caret', 'pcaMethods', 'tidyverse', 'MetaboAnalystR', 'tinytex',
      'HotellingEllipse', 'ggforce', 'tools', 'cowplot', 'metaboPipe'
    )
  )
  
  
  #### Global variables #####
  # General config
  outdir = 'Results'
  dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
  outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility
  
  list(
  
  
  
  
  
  )
"
  directory <- getwd()
  writeLines(code, con = file.path(directory, '_targets.R'))
  
  message("Created _targets.R file in ", directory)
  
  targets::tar_assert_package("usethis")
  usethis::edit_file(path = paste0(directory,"/", '_targets.R'), open = TRUE)
}


#' Run Shiny App
#'
#' This function launches the Shiny app included with the package.
#'
#' @export
#' @examples
#' pipePilers()
pipePilers <- function() {
  # shinyenv <- new.env()
  wd <- getwd()
  # assign("wd", wd, envir = shinyenv)
  appDir <- system.file("shiny", package = "metaboPipe", mustWork = TRUE)
  if (appDir == "") {
    stop("Could not find Shiny app directory. Please re-install the package.")
  }
  # shiny::loadSupport(appDir = appDir)
  shinyOptions(wd = wd)
  shiny::shinyAppDir(appDir)
}



----
Work\Work.Rproj
Version: 1.0

RestoreWorkspace: Default
SaveWorkspace: Default
AlwaysSaveHistory: Default

EnableCodeIndexing: Yes
UseSpacesForTab: Yes
NumSpacesForTab: 2
Encoding: UTF-8

RnwWeave: Sweave
LaTeX: XeLaTeX

BuildType: Package
PackageUseDevtools: Yes
PackageInstallArgs: --no-multiarch --with-keep.source
PackageRoxygenize: rd,collate,namespace,vignette

----
Work\inst\_targets.R
# _targets.R file:
library(targets)
library(tarchetypes)
library(crew)
# library(doParallel)
# Load all R scripts in the R/ directory.
file.sources <- list.files("R", pattern = "*.R", full.names = TRUE)
invisible(sapply(file.sources, source, .GlobalEnv))

# Declare libraries
# These are the libraries that the pipeline depends on.

# To create a list with all the libraries to copy-paste:
# cat(paste(shQuote(unique(renv::dependencies(path = "R")$Package), type="cmd"), collapse=", "))

tar_option_set(
  packages = c(
    "structToolbox", "SummarizedExperiment", "VIM", "impute", "imputeLCMD",
    "missForest", "caret", "pcaMethods", "tidyverse", "MetaboAnalystR", "tinytex",
    "HotellingEllipse", "ggforce", "tools", "cowplot", "metaboPipe"
  )
)
# Declare controller
# Create a controller with 5 workers and a 3-second idle time.
# controller <- crew::crew_controller_local(
#   name = "Controller",
#   workers = 2,
#   seconds_idle = 3
# )
# tar_option_set(controller = controller)
#### Global variables #####
out_dir <- './Output'
dataMatrixPath <- 'data/ST000284/dataMatrix.csv'
sampleMetadataPath <- 'data/ST000284/sampleMetadata.csv'
variableMetadataPath <- 'data/ST000284/variableMetadata.csv'
dataSep <- ','
sampleSep <- ','
variableSep <- ','

# Columns settings
factor_cols <- c('Groups', 'Age', 'Gender', 'Smoking')
sample_id_col <- 'sample_id'
sample_type_col <- 'sample_type'
group_col <- 'Groups'


dir.create(out_dir, showWarnings = FALSE)
out_dir <- tools::file_path_as_absolute(out_dir)
##### DEFINE THE PIPELINE ######
list(
# LOAD THE DATA
load_data(data, dataMatrixPath, sampleMetadataPath, dataSep = dataSep, sampleSep = sampleSep, variableSep = variableSep),

# Create a DatasetExperiment object
createExperiment(experiment, data),

# Factorize the cols
factorize_cols(factorized, experiment, factor_cols),

#SHINY STEPS

filter_step(filtered_1, factorized, threshold = 0.8, filter_outliers = TRUE, conf.limit = '0.95', out_dir = out_dir),
impute(imputed_2, filtered_1, method = 'RF', k = 5),
normalize(normalized_3, imputed_2, factor_col = group_col, sample_id_col = sample_id_col, rowNorm = 'QuantileNorm', transNorm = 'NULL', scaleNorm = 'NULL', ref = '', out_dir = out_dir),
#### EXTRACTION ####
# Extract the data
tar_target(extract_data, export_data(normalized_3, out_dir = out_dir, out_name = 'Processed')),


#### Cleaning ####
tar_target(clean, withr::with_dir(out_dir, unlink('TempData', recursive = TRUE)))
)

----
Work\inst\_targets_template.R
# _targets.R file:
library(targets)
library(tarchetypes)
library(crew)
library(metaboPipe)
# library(doParallel)
# Load all R scripts in the R/ directory.
# file.sources <- list.files("R", pattern = "*.R", full.names = TRUE)
# invisible(sapply(file.sources, source, .GlobalEnv))

# Declare libraries
# These are the libraries that the pipeline depends on.

# To create a list with all the libraries to copy-paste:
# cat(paste(shQuote(unique(renv::dependencies(path = "R")$Package), type="cmd"), collapse=", "))

tar_option_set(
  packages = c(
    "structToolbox", "SummarizedExperiment", "VIM", "impute", "imputeLCMD",
    "missForest", "caret", "pcaMethods", "tidyverse", "MetaboAnalystR", "tinytex",
    "HotellingEllipse", "ggforce", "tools", "cowplot", "metaboPipe"
  )
)
# Declare controller
# Create a controller with 5 workers and a 3-second idle time.
# controller <- crew::crew_controller_local(
#   name = "Controller",
#   workers = 2,
#   seconds_idle = 3
# )
# tar_option_set(controller = controller)

----
Work\inst\app.R
library(shiny)
library(DT)
library(shinyFiles)
# Define UI
ui <- navbarPage(
  id = "DataInitializer",
  title = "MetaboPipe Data Initializer",
  tabPanel(
    "Data Upload",
    sidebarLayout(
      sidebarPanel(
        selectInput("dataset", "Select Dataset:",
          choices = c("MTBLS79", "ST000284", "Upload data"),
          selected = "Upload data"
        ),
        conditionalPanel(
          condition = "input.dataset == 'Upload data'",
          fileInput("dataMatrix", "Choose DataMatrix file",
            accept = c(".csv")
          ),
          radioButtons("dataSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          ),
          tags$hr(),
          fileInput("sampleMetadata", "Choose Sample Metadata file",
            accept = c(".csv")
          ),
          radioButtons("sampleSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          ),
          tags$hr(),
          fileInput("variableMetadata", "Choose Variable Metadata file",
            accept = c(".csv")
          ),
          radioButtons("variableSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          )
        ),
        actionButton("loadData", "Load Data", style = "color: #fff; background-color: #28a745; border-color: #28a745;")
      ),
      mainPanel()
    )
  ),
  tabPanel(
    "DataConfig",
    sidebarLayout(
      sidebarPanel(
        selectizeInput("factorCols", "Select columns as factor variables:",
          choices = NULL,
          multiple = TRUE
        ),
        selectizeInput("sampleIdCol", "Select sample ID column:",
          choices = NULL
        ),
        selectizeInput("sampleTypeCol", "Select sample Type column (if its QC, Blank or Sample):",
          choices = NULL
        ),
        selectizeInput("groupCol", "Select the Group column (the study variable):",
          choices = NULL
        ),
        actionButton("setSettings", "Set Settings", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      ),
      mainPanel(
        tabsetPanel(
          tabPanel("Data Summary", DT::dataTableOutput("dataMatrixTable")),
          tabPanel("Sample Metadata Summary", DT::dataTableOutput("sampleMetadataTable")),
          tabPanel("Variable Metadata Summary", DT::dataTableOutput("variableMetadataTable"))
        )
      )
    )
  ),
  tabPanel(
    "Process Selector",
    sidebarLayout(
      sidebarPanel(
        selectInput("process", "Select the first step to perform:",
          choices = c("Filter", "Impute", "Normalize", "Batch Correct")
        ),
        conditionalPanel(
          condition = "input.process == 'Filter'",
          numericInput("naThreshold", "Select the threshold for missing values (between 0 and 1):",
            min = 0, max = 1, value = 0.80, step = 0.01
          ),
          checkboxInput("filterOutliers", "Filter Outliers"),
          conditionalPanel(
            condition = "input.filterOutliers == true",
            selectInput("confLimit", "Select the confidence limit for outlier removal:",
              choices = c("0.95", "0.99")
            )
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Impute'",
          selectInput("imputeMethod", "Select the imputation method:",
            choices = c("mean", "median", "RF", "QRILC", "kNN", "SVD", "bpca", "ppca"),
            selected = "RF"
          ),
          conditionalPanel(
            condition = "input.imputeMethod == 'kNN'",
            numericInput("k", "Select the number of neighbors for kNN imputation:",
              min = 1, value = 5
            )
          ),
          conditionalPanel(
            condition = "input.imputeMethod == 'SVD' || input.imputeMethod == 'BPCA' || input.imputeMethod == 'PPCA'",
            numericInput("k", "Select the number of principal components for imputation:",
              min = 1, value = 5
            )
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Normalize'",
          selectInput("rowNormMethod", "Select the row-wise normalization method:",
            choices = c("QuantileNorm", "CompNorm", "SumNorm", "MedianNorm", "SpecNorm", "NULL"),
            selected = "NULL"
          ),
          conditionalPanel(
            condition = "input.rowNormMethod == 'CompNorm'",
            selectizeInput("ref", "Enter the compound of reference for CompNorm normalization:",
              choices = NULL
            )
          ),
          selectInput("transNormMethod", "Select the transformation method:",
            choices = c("LogNorm", "CrNorm", "NULL"),
            selected = "NULL"
          ),
          selectInput("scaleNormMethod", "Select the scaling method:",
            choices = c("MeanCenter", "AutoNorm", "ParetoNorm", "RangeNorm", "NULL"),
            selected = "NULL"
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Batch Correct'",
          selectInput("batchCorrectMethod", "Select the batch correction method:",
            choices = c("ComBat", "SVA", "RUV", "NULL"),
            selected = "NULL"
          ),
          selectizeInput("orderCol", "Select the Order column (the injection order):",
            choices = NULL
          ),
          selectizeInput("batchCol", "Select the batch column:",
            choices = NULL
          ),
        ),
        actionButton("removeStep", "Remove Step", style = "color: #fff; background-color: #dc3545; border-color: #dc3545;"),
        actionButton("addStep", "Add Step", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      ),
      mainPanel(
        h3("Added Steps"),
        verbatimTextOutput("addedSteps"),
        shinyDirButton("outDir", "Output Directory", "Select a directory", "Choose", icon = icon("folder", class = "solid", lib = "font-awesome")),
        actionButton("writeSteps", "Write Steps", style = "color: #fff; background-color: #28a745; border-color: #28a745;"),
        actionButton("runPipeline", "Run Pipeline", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      )
    )
  )
)

# Define server logic
server <- function(input, output, session) {
  
  
  data <- reactiveValues(
    dataMatrixPath = NULL,
    sampleMetadataPath = NULL,
    variableMetadataPath = NULL,
    dataSeparator = ",",
    sampleSeparator = ",",
    variableSeparator = ",",
    factorCols = NULL,
    sampleIdCol = NULL,
    sampleTypeCol = NULL,
    groupCol = NULL,
    orderCol = NULL,
    batchCol = NULL,
    rowNormMethod = NULL,
    transNormMethod = NULL,
    scaleNormMethod = NULL,
    confLimit = NULL,
    naThreshold = 0.80,
    filterOutliers = FALSE,
    imputeMethod = "Random Forest",
    k = 5,
    batchCorrectMethod = NULL,
    ref = NULL,
    outDir = "output"
  )

  observeEvent(input$loadData, {
    if (input$dataset == "Upload data") {
      data$dataMatrixPath <- input$dataMatrix$datapath
      data$sampleMetadataPath <- input$sampleMetadata$datapath
      data$variableMetadataPath <- input$variableMetadata$datapath
    } else {
      data$dataMatrixPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/data.csv",
        "ST000284" = "data/ST000284/dataMatrix.csv"
      )
      data$sampleMetadataPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/sample_meta.csv",
        "ST000284" = "data/ST000284/sampleMetadata.csv"
      )
      data$variableMetadataPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/variable_meta.csv",
        "ST000284" = "data/ST000284/variableMetadata.csv"
      )
      data$dataSeparator <- ","
      data$sampleSeparator <- ","
      data$variableSeparator <- ","
    }
  })

  observeEvent(input$dataSep, {
    data$dataSeparator <- input$dataSep
  })
  observeEvent(input$sampleSep, {
    data$sampleSeparator <- input$sampleSep
  })
  observeEvent(input$variableSep, {
    data$variableSeparator <- input$variableSep
  })

  observeEvent(data$sampleMetadataPath, {
    req(data$sampleMetadataPath)
    df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator, strip.white = TRUE)
    updateSelectizeInput(session, "factorCols", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "sampleIdCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "sampleTypeCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "groupCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "orderCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "batchCol", choices = colnames(df), server = TRUE)
  })

  observeEvent(input$setSettings, {
    data$factorCols <- input$factorCols
    data$sampleIdCol <- input$sampleIdCol
    data$sampleTypeCol <- input$sampleTypeCol
    data$groupCol <- input$groupCol
  })

  observeEvent(input$rowNormMethod, {
    if (input$rowNormMethod == "CompNorm") {
      req(data$dataMatrixPath)
      df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator, strip.white = TRUE)
      updateSelectizeInput(session, "ref", choices = colnames(df), server = TRUE)
    }
  })

  output$dataMatrixTable <- DT::renderDataTable({
    req(data$dataMatrixPath)
    df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator)
    DT::datatable(df)
  })

  output$sampleMetadataTable <- DT::renderDataTable({
    req(data$sampleMetadataPath)
    df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator)
    DT::datatable(df)
  })

  output$variableMetadataTable <- DT::renderDataTable({
    req(data$variableMetadataPath)
    df <- read.csv(data$variableMetadataPath, sep = data$variableSeparator)
    DT::datatable(df)
  })

  output$addedSteps <- renderPrint({
    step_lines$lines
  })



  # Reactive values to store lines to add
  step_lines <- reactiveValues(lines = character(0))
  previous_target_name <- reactiveValues(name = "factorized")
  step_counter <- reactiveValues(counter = 1)



  observeEvent(input$addStep, {
    # Set the targets file
    if (step_counter$counter == 1) {
      previous_target_name$name <- "factorized"
    }

    if (input$process == "Filter") {
      target_name <- paste0("filtered_", step_counter$counter)
      line_to_add <- paste0("filter_step(", target_name, ", ", previous_target_name$name, ", threshold = ", input$naThreshold, ", filter_outliers = ", input$filterOutliers, ", conf.limit = '", input$confLimit, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Impute") {
      target_name <- paste0("imputed_", step_counter$counter)
      line_to_add <- paste0("impute(", target_name, ", ", previous_target_name$name, ", method = '", input$imputeMethod, "', k = ", input$k, "),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Normalize") {
      target_name <- paste0("normalized_", step_counter$counter)
      line_to_add <- paste0("normalize(", target_name, ", ", previous_target_name$name, ", factor_col = group_col, sample_id_col = sample_id_col, rowNorm = '", input$rowNormMethod, "', transNorm = '", input$transNormMethod, "', scaleNorm = '", input$scaleNormMethod, "', ref = '", input$ref, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Batch Correct") {
      target_name <- paste0("batch_corrected_", step_counter$counter)
      line_to_add <- paste0("batch_correct(", target_name, ", ", previous_target_name$name, ", method = '", input$batchCorrectMethod, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    previous_target_name$name <- target_name
    step_counter$counter <- step_counter$counter + 1
  })

  # Remove last step
  observeEvent(input$removeStep, {
    # Check if there are steps to remove
    if (length(step_lines$lines) > 0) {
      # Remove the last step added
      step_lines$lines <- step_lines$lines[-length(step_lines$lines)]
      # Decrement the step counter if greater than 1
      if (step_counter$counter > 1) {
        step_counter$counter <- step_counter$counter - 1
      }
    }
  })

  observeEvent(input$writeSteps, {
    # Transform the outDir to a valid path
    out_dir <- parseDirPath(directories, input$outDir)

    # Print out the variables being concatenated

    # Write lines to file
    targets_file <- "_targets_template.R"
    template_lines <- readLines(targets_file)
    # Add the lines to the beginning of the file
    global_var_lines <- c(
      "#### Global variables #####",
      paste0("out_dir <- '", out_dir, "'"),
      paste0("dataMatrixPath <- '", data$dataMatrixPath, "'"),
      paste0("sampleMetadataPath <- '", data$sampleMetadataPath, "'"),
      paste0("variableMetadataPath <- '", data$variableMetadataPath, "'"),
      paste0("dataSep <- '", data$dataSeparator, "'"),
      paste0("sampleSep <- '", data$sampleSeparator, "'"),
      paste0("variableSep <- '", data$variableSeparator, "'"),
      "",
      "# Columns settings",
      paste0("factor_cols <- c(", paste0("'", data$factorCols, "'", collapse = ", "), ")"),
      paste0("sample_id_col <- '", data$sampleIdCol, "'"),
      paste0("sample_type_col <- '", data$sampleTypeCol, "'"),
      paste0("group_col <- '", data$groupCol, "'"),
      "",
      "",
      "dir.create(out_dir, showWarnings = FALSE)",
      "out_dir <- tools::file_path_as_absolute(out_dir)"
    )

    pipeline_load_lines <- c(
      "##### DEFINE THE PIPELINE ######",
      "list(",
      "# LOAD THE DATA",
      "load_data(data, dataMatrixPath, sampleMetadataPath, dataSep = dataSep, sampleSep = sampleSep, variableSep = variableSep),",
      "",
      "# Create a DatasetExperiment object",
      "createExperiment(experiment, data),",
      "",
      "# Factorize the cols",
      "factorize_cols(factorized, experiment, factor_cols),",
      "",
      "#SHINY STEPS",
      ""
    )

    pipeline_ending_lines <- c(
      "#### EXTRACTION ####",
      "# Extract the data",
      paste0("tar_target(extract_data, export_data(", previous_target_name$name, ", out_dir = out_dir, out_name = 'Processed')),"),
      "",
      "",
      "#### Cleaning ####",
      "tar_target(clean, withr::with_dir(out_dir, unlink('TempData', recursive = TRUE)))",
      ")"
    )

    lines <- c(template_lines, global_var_lines, pipeline_load_lines, step_lines$lines, pipeline_ending_lines)

    writeLines(lines, "_targets.R")
  })

  volumes <- getVolumes()()
  directories <- c(wd = ".", dirUp = "..", volumes)
  # Function to choose output directory
  shinyDirChoose(input, "outDir", roots = directories, filetypes = c("", "txt"))


  observeEvent(input$runPipeline, {
    validate(
      need(data$dataMatrixPath != "", "Please upload the data matrix file."),
      need(data$sampleMetadataPath != "", "Please upload the sample metadata file."),
      need(input$outDir != "", "Please select an output directory."),
      need(data$factorCols != "", "Please select the factor columns."),
      need(data$sampleIdCol != "", "Please select the sample ID column."),
      need(data$sampleTypeCol != "", "Please select the sample type column."),
      need(data$groupCol != "", "Please select the group column.")
    )
    # Run the pipeline with error handling
    tryCatch(
      {
        library(targets)
        tar_make()
      },
      error = function(e) {
        # Print the error message
        print(paste("Error:", e$message))
        # Display a notification or message to the user
        showModal(modalDialog(
          title = "Error",
          "An error occurred. Please check your inputs and try again.",
          easyClose = TRUE
        ))
      }
    )
  })
}
# Run the application
shinyApp(ui = ui, server = server)

----
Work\inst\app.bak
library(shiny)
library(DT)
library(shinyFiles)
# Define UI
ui <- navbarPage(
  id = "DataInitializer",
  title = "MetaboPipe Data Initializer",
  tabPanel(
    "Data Upload",
    sidebarLayout(
      sidebarPanel(
        selectInput("dataset", "Select Dataset:",
          choices = c("MTBLS79", "ST000284", "Upload data"),
          selected = "Upload data"
        ),
        conditionalPanel(
          condition = "input.dataset == 'Upload data'",
          fileInput("dataMatrix", "Choose DataMatrix file",
            accept = c(".csv")
          ),
          radioButtons("dataSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          ),
          tags$hr(),
          fileInput("sampleMetadata", "Choose Sample Metadata file",
            accept = c(".csv")
          ),
          radioButtons("sampleSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          ),
          tags$hr(),
          fileInput("variableMetadata", "Choose Variable Metadata file",
            accept = c(".csv")
          ),
          radioButtons("variableSep", "Separator",
            choices = c(
              Comma = ",",
              Semicolon = ";"
            ),
            selected = ","
          )
        ),
        actionButton("loadData", "Load Data", style = "color: #fff; background-color: #28a745; border-color: #28a745;")
      ),
      mainPanel()
    )
  ),
  tabPanel(
    "DataConfig",
    sidebarLayout(
      sidebarPanel(
        selectizeInput("factorCols", "Select columns as factor variables:",
          choices = NULL,
          multiple = TRUE
        ),
        selectizeInput("sampleIdCol", "Select sample ID column:",
          choices = NULL
        ),
        selectizeInput("sampleTypeCol", "Select sample Type column (if its QC, Blank or Sample):",
          choices = NULL
        ),
        selectizeInput("groupCol", "Select the Group column (the study variable):",
          choices = NULL
        ),
        actionButton("setSettings", "Set Settings", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      ),
      mainPanel(
        tabsetPanel(
          tabPanel("Data Summary", DT::dataTableOutput("dataMatrixTable")),
          tabPanel("Sample Metadata Summary", DT::dataTableOutput("sampleMetadataTable")),
          tabPanel("Variable Metadata Summary", DT::dataTableOutput("variableMetadataTable"))
        )
      )
    )
  ),
  tabPanel(
    "Process Selector",
    sidebarLayout(
      sidebarPanel(
        selectInput("process", "Select the first step to perform:",
          choices = c("Filter", "Impute", "Normalize", "Batch Correct")
        ),
        conditionalPanel(
          condition = "input.process == 'Filter'",
          numericInput("naThreshold", "Select the threshold for missing values (between 0 and 1):",
            min = 0, max = 1, value = 0.80, step = 0.01
          ),
          checkboxInput("filterOutliers", "Filter Outliers"),
          conditionalPanel(
            condition = "input.filterOutliers == true",
            selectInput("confLimit", "Select the confidence limit for outlier removal:",
              choices = c("0.95", "0.99")
            )
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Impute'",
          selectInput("imputeMethod", "Select the imputation method:",
            choices = c("mean", "median", "RF", "QRILC", "kNN", "SVD", "bpca", "ppca"),
            selected = "RF"
          ),
          conditionalPanel(
            condition = "input.imputeMethod == 'kNN'",
            numericInput("k", "Select the number of neighbors for kNN imputation:",
              min = 1, value = 5
            )
          ),
          conditionalPanel(
            condition = "input.imputeMethod == 'SVD' || input.imputeMethod == 'BPCA' || input.imputeMethod == 'PPCA'",
            numericInput("k", "Select the number of principal components for imputation:",
              min = 1, value = 5
            )
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Normalize'",
          selectInput("rowNormMethod", "Select the row-wise normalization method:",
            choices = c("QuantileNorm", "CompNorm", "SumNorm", "MedianNorm", "SpecNorm", "NULL"),
            selected = "NULL"
          ),
          conditionalPanel(
            condition = "input.rowNormMethod == 'CompNorm'",
            selectizeInput("ref", "Enter the compound of reference for CompNorm normalization:",
              choices = NULL
            )
          ),
          selectInput("transNormMethod", "Select the transformation method:",
            choices = c("LogNorm", "CrNorm", "NULL"),
            selected = "NULL"
          ),
          selectInput("scaleNormMethod", "Select the scaling method:",
            choices = c("MeanCenter", "AutoNorm", "ParetoNorm", "RangeNorm", "NULL"),
            selected = "NULL"
          )
        ),
        conditionalPanel(
          condition = "input.process == 'Batch Correct'",
          selectInput("batchCorrectMethod", "Select the batch correction method:",
            choices = c("ComBat", "SVA", "RUV", "NULL"),
            selected = "NULL"
          ),
          selectizeInput("orderCol", "Select the Order column (the injection order):",
            choices = NULL
          ),
          selectizeInput("batchCol", "Select the batch column:",
            choices = NULL
          ),
        ),
        actionButton("removeStep", "Remove Step", style = "color: #fff; background-color: #dc3545; border-color: #dc3545;"),
        actionButton("addStep", "Add Step", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      ),
      mainPanel(
        h3("Added Steps"),
        verbatimTextOutput("addedSteps"),
        shinyDirButton("outDir", "Output Directory", "Select a directory", "Choose", icon = icon("folder", class = "solid", lib = "font-awesome")),
        actionButton("writeSteps", "Write Steps", style = "color: #fff; background-color: #28a745; border-color: #28a745;"),
        actionButton("runPipeline", "Run Pipeline", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
      )
    )
  )
)

# Define server logic
server <- function(input, output, session) {
  
  
  data <- reactiveValues(
    dataMatrixPath = NULL,
    sampleMetadataPath = NULL,
    variableMetadataPath = NULL,
    dataSeparator = ",",
    sampleSeparator = ",",
    variableSeparator = ",",
    factorCols = NULL,
    sampleIdCol = NULL,
    sampleTypeCol = NULL,
    groupCol = NULL,
    orderCol = NULL,
    batchCol = NULL,
    rowNormMethod = NULL,
    transNormMethod = NULL,
    scaleNormMethod = NULL,
    confLimit = NULL,
    naThreshold = 0.80,
    filterOutliers = FALSE,
    imputeMethod = "Random Forest",
    k = 5,
    batchCorrectMethod = NULL,
    ref = NULL,
    outDir = "output"
  )

  observeEvent(input$loadData, {
    if (input$dataset == "Upload data") {
      data$dataMatrixPath <- input$dataMatrix$datapath
      data$sampleMetadataPath <- input$sampleMetadata$datapath
      data$variableMetadataPath <- input$variableMetadata$datapath
    } else {
      data$dataMatrixPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/data.csv",
        "ST000284" = "data/ST000284/dataMatrix.csv"
      )
      data$sampleMetadataPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/sample_meta.csv",
        "ST000284" = "data/ST000284/sampleMetadata.csv"
      )
      data$variableMetadataPath <- switch(input$dataset,
        "MTBLS79" = "data/MTBLS79/variable_meta.csv",
        "ST000284" = "data/ST000284/variableMetadata.csv"
      )
      data$dataSeparator <- ","
      data$sampleSeparator <- ","
      data$variableSeparator <- ","
    }
  })

  observeEvent(input$dataSep, {
    data$dataSeparator <- input$dataSep
  })
  observeEvent(input$sampleSep, {
    data$sampleSeparator <- input$sampleSep
  })
  observeEvent(input$variableSep, {
    data$variableSeparator <- input$variableSep
  })

  observeEvent(data$sampleMetadataPath, {
    req(data$sampleMetadataPath)
    df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator, strip.white = TRUE)
    updateSelectizeInput(session, "factorCols", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "sampleIdCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "sampleTypeCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "groupCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "orderCol", choices = colnames(df), server = TRUE)
    updateSelectizeInput(session, "batchCol", choices = colnames(df), server = TRUE)
  })

  observeEvent(input$setSettings, {
    data$factorCols <- input$factorCols
    data$sampleIdCol <- input$sampleIdCol
    data$sampleTypeCol <- input$sampleTypeCol
    data$groupCol <- input$groupCol
  })

  observeEvent(input$rowNormMethod, {
    if (input$rowNormMethod == "CompNorm") {
      req(data$dataMatrixPath)
      df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator, strip.white = TRUE)
      updateSelectizeInput(session, "ref", choices = colnames(df), server = TRUE)
    }
  })

  output$dataMatrixTable <- DT::renderDataTable({
    req(data$dataMatrixPath)
    df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator)
    DT::datatable(df)
  })

  output$sampleMetadataTable <- DT::renderDataTable({
    req(data$sampleMetadataPath)
    df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator)
    DT::datatable(df)
  })

  output$variableMetadataTable <- DT::renderDataTable({
    req(data$variableMetadataPath)
    df <- read.csv(data$variableMetadataPath, sep = data$variableSeparator)
    DT::datatable(df)
  })

  output$addedSteps <- renderPrint({
    step_lines$lines
  })



  # Reactive values to store lines to add
  step_lines <- reactiveValues(lines = character(0))
  previous_target_name <- reactiveValues(name = "factorized")
  step_counter <- reactiveValues(counter = 1)



  observeEvent(input$addStep, {
    # Set the targets file
    if (step_counter$counter == 1) {
      previous_target_name$name <- "factorized"
    }

    if (input$process == "Filter") {
      target_name <- paste0("filtered_", step_counter$counter)
      line_to_add <- paste0("filter_step(", target_name, ", ", previous_target_name$name, ", threshold = ", input$naThreshold, ", filter_outliers = ", input$filterOutliers, ", conf.limit = '", input$confLimit, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Impute") {
      target_name <- paste0("imputed_", step_counter$counter)
      line_to_add <- paste0("impute(", target_name, ", ", previous_target_name$name, ", method = '", input$imputeMethod, "', k = ", input$k, "),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Normalize") {
      target_name <- paste0("normalized_", step_counter$counter)
      line_to_add <- paste0("normalize(", target_name, ", ", previous_target_name$name, ", factor_col = group_col, sample_id_col = sample_id_col, rowNorm = '", input$rowNormMethod, "', transNorm = '", input$transNormMethod, "', scaleNorm = '", input$scaleNormMethod, "', ref = '", input$ref, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    if (input$process == "Batch Correct") {
      target_name <- paste0("batch_corrected_", step_counter$counter)
      line_to_add <- paste0("batch_correct(", target_name, ", ", previous_target_name$name, ", method = '", input$batchCorrectMethod, "', out_dir = out_dir),")

      # Append line to reactiveValues
      step_lines$lines <- c(step_lines$lines, line_to_add)
    }

    previous_target_name$name <- target_name
    step_counter$counter <- step_counter$counter + 1
  })

  # Remove last step
  observeEvent(input$removeStep, {
    # Check if there are steps to remove
    if (length(step_lines$lines) > 0) {
      # Remove the last step added
      step_lines$lines <- step_lines$lines[-length(step_lines$lines)]
      # Decrement the step counter if greater than 1
      if (step_counter$counter > 1) {
        step_counter$counter <- step_counter$counter - 1
      }
    }
  })

  observeEvent(input$writeSteps, {
    # Transform the outDir to a valid path
    out_dir <- parseDirPath(directories, input$outDir)

    # Print out the variables being concatenated

    # Write lines to file
    targets_file <- "_targets_template.R"
    template_lines <- readLines(targets_file)
    # Add the lines to the beginning of the file
    global_var_lines <- c(
      "#### Global variables #####",
      paste0("out_dir <- '", out_dir, "'"),
      paste0("dataMatrixPath <- '", data$dataMatrixPath, "'"),
      paste0("sampleMetadataPath <- '", data$sampleMetadataPath, "'"),
      paste0("variableMetadataPath <- '", data$variableMetadataPath, "'"),
      paste0("dataSep <- '", data$dataSeparator, "'"),
      paste0("sampleSep <- '", data$sampleSeparator, "'"),
      paste0("variableSep <- '", data$variableSeparator, "'"),
      "",
      "# Columns settings",
      paste0("factor_cols <- c(", paste0("'", data$factorCols, "'", collapse = ", "), ")"),
      paste0("sample_id_col <- '", data$sampleIdCol, "'"),
      paste0("sample_type_col <- '", data$sampleTypeCol, "'"),
      paste0("group_col <- '", data$groupCol, "'"),
      "",
      "",
      "dir.create(out_dir, showWarnings = FALSE)",
      "out_dir <- tools::file_path_as_absolute(out_dir)"
    )

    pipeline_load_lines <- c(
      "##### DEFINE THE PIPELINE ######",
      "list(",
      "# LOAD THE DATA",
      "load_data(data, dataMatrixPath, sampleMetadataPath, dataSep = dataSep, sampleSep = sampleSep, variableSep = variableSep),",
      "",
      "# Create a DatasetExperiment object",
      "createExperiment(experiment, data),",
      "",
      "# Factorize the cols",
      "factorize_cols(factorized, experiment, factor_cols),",
      "",
      "#SHINY STEPS",
      ""
    )

    pipeline_ending_lines <- c(
      "#### EXTRACTION ####",
      "# Extract the data",
      paste0("tar_target(extract_data, export_data(", previous_target_name$name, ", out_dir = out_dir, out_name = 'Processed')),"),
      "",
      "",
      "#### Cleaning ####",
      "tar_target(clean, withr::with_dir(out_dir, unlink('TempData', recursive = TRUE)))",
      ")"
    )

    lines <- c(template_lines, global_var_lines, pipeline_load_lines, step_lines$lines, pipeline_ending_lines)

    writeLines(lines, "_targets.R")
  })

  volumes <- getVolumes()()
  directories <- c(wd = ".", dirUp = "..", volumes)
  # Function to choose output directory
  shinyDirChoose(input, "outDir", roots = directories, filetypes = c("", "txt"))


  observeEvent(input$runPipeline, {
    validate(
      need(data$dataMatrixPath != "", "Please upload the data matrix file."),
      need(data$sampleMetadataPath != "", "Please upload the sample metadata file."),
      need(input$outDir != "", "Please select an output directory."),
      need(data$factorCols != "", "Please select the factor columns."),
      need(data$sampleIdCol != "", "Please select the sample ID column."),
      need(data$sampleTypeCol != "", "Please select the sample type column."),
      need(data$groupCol != "", "Please select the group column.")
    )
    # Run the pipeline with error handling
    tryCatch(
      {
        library(targets)
        tar_make()
      },
      error = function(e) {
        # Print the error message
        print(paste("Error:", e$message))
        # Display a notification or message to the user
        showModal(modalDialog(
          title = "Error",
          "An error occurred. Please check your inputs and try again.",
          easyClose = TRUE
        ))
      }
    )
  })
}
# Run the application
shinyApp(ui = ui, server = server)

----
Work\inst\shiny\server.R
library(shiny)
library(DT)
library(shinyFiles)

function(input, output, session) {
    # Static values
    example_data_path <- system.file("data", package = "metaboPipe")
    targets_template <- system.file("_targets_template.R", package = "metaboPipe")

    wd <- getShinyOption("wd", default = ".")
    setwd(wd)


    data <- reactiveValues(
        dataMatrixPath = NULL,
        sampleMetadataPath = NULL,
        variableMetadataPath = NULL,
        dataSeparator = ",",
        sampleSeparator = ",",
        variableSeparator = ",",
        factorCols = NULL,
        sampleIdCol = NULL,
        sampleTypeCol = NULL,
        groupCol = NULL,
        orderCol = NULL,
        batchCol = NULL,
        rowNormMethod = NULL,
        transNormMethod = NULL,
        scaleNormMethod = NULL,
        confLimit = NULL,
        naThreshold = 0.80,
        filterOutliers = FALSE,
        imputeMethod = "Random Forest",
        k = 5,
        batchCorrectMethod = NULL,
        ref = NULL,
        outDir = "output"
    )

    observeEvent(input$loadData, {
        if (input$dataset == "Upload data") {
            data$dataMatrixPath <- input$dataMatrix$datapath
            data$sampleMetadataPath <- input$sampleMetadata$datapath
            data$variableMetadataPath <- input$variableMetadata$datapath
        } else {
            data$dataMatrixPath <- switch(input$dataset,
                "MTBLS79" = paste0(example_data_path, "/MTBLS79/data.csv"),
                "ST000284" = paste0(example_data_path, "/ST000284/dataMatrix.csv")
            )
            data$sampleMetadataPath <- switch(input$dataset,
                "MTBLS79" = paste0(example_data_path, "/MTBLS79/sample_meta.csv"),
                "ST000284" = paste0(example_data_path, "/ST000284/sampleMetadata.csv")
            )
            data$variableMetadataPath <- switch(input$dataset,
                "MTBLS79" = paste0(example_data_path, "/MTBLS79/variable_meta.csv"),
                "ST000284" = paste0(example_data_path, "/ST000284/variableMetadata.csv")
            )
            data$dataSeparator <- ","
            data$sampleSeparator <- ","
            data$variableSeparator <- ","
        }
    })

    observeEvent(input$dataSep, {
        data$dataSeparator <- input$dataSep
    })
    observeEvent(input$sampleSep, {
        data$sampleSeparator <- input$sampleSep
    })
    observeEvent(input$variableSep, {
        data$variableSeparator <- input$variableSep
    })

    observeEvent(data$sampleMetadataPath, {
        req(data$sampleMetadataPath)
        df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator, strip.white = TRUE)
        updateSelectizeInput(session, "factorCols", choices = colnames(df), server = TRUE)
        updateSelectizeInput(session, "sampleIdCol", choices = colnames(df), server = TRUE)
        updateSelectizeInput(session, "sampleTypeCol", choices = colnames(df), server = TRUE)
        updateSelectizeInput(session, "groupCol", choices = colnames(df), server = TRUE)
        updateSelectizeInput(session, "orderCol", choices = colnames(df), server = TRUE)
        updateSelectizeInput(session, "batchCol", choices = colnames(df), server = TRUE)
    })

    observeEvent(input$setSettings, {
        data$factorCols <- input$factorCols
        data$sampleIdCol <- input$sampleIdCol
        data$sampleTypeCol <- input$sampleTypeCol
        data$groupCol <- input$groupCol
    })

    observeEvent(input$rowNormMethod, {
        if (input$rowNormMethod == "CompNorm") {
            req(data$dataMatrixPath)
            df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator, strip.white = TRUE)
            updateSelectizeInput(session, "ref", choices = colnames(df), server = TRUE)
        }
    })

    output$dataMatrixTable <- DT::renderDataTable({
        req(data$dataMatrixPath)
        df <- read.csv(data$dataMatrixPath, sep = data$dataSeparator)
        DT::datatable(df)
    })

    output$sampleMetadataTable <- DT::renderDataTable({
        req(data$sampleMetadataPath)
        df <- read.csv(data$sampleMetadataPath, sep = data$sampleSeparator)
        DT::datatable(df)
    })

    output$variableMetadataTable <- DT::renderDataTable({
        req(data$variableMetadataPath)
        df <- read.csv(data$variableMetadataPath, sep = data$variableSeparator)
        DT::datatable(df)
    })

    output$addedSteps <- renderPrint({
        step_lines$lines
    })

    # Reactive values to store lines to add
    step_lines <- reactiveValues(lines = character(0))
    previous_target_name <- reactiveValues(name = "factorized")
    step_counter <- reactiveValues(counter = 1)

    observeEvent(input$addStep, {
        # Set the targets file
        if (step_counter$counter == 1) {
            previous_target_name$name <- "factorized"
        }

        if (input$process == "Filter") {
            target_name <- paste0("filtered_", step_counter$counter)
            line_to_add <- paste0("filter_step(", target_name, ", ", previous_target_name$name, ", threshold = ", input$naThreshold, ", filter_outliers = ", input$filterOutliers, ", conf.limit = '", input$confLimit, "', out_dir = out_dir),")

            # Append line to reactiveValues
            step_lines$lines <- c(step_lines$lines, line_to_add)
        }

        if (input$process == "Impute") {
            target_name <- paste0("imputed_", step_counter$counter)
            line_to_add <- paste0("impute(", target_name, ", ", previous_target_name$name, ", method = '", input$imputeMethod, "', k = ", input$k, "),")

            # Append line to reactiveValues
            step_lines$lines <- c(step_lines$lines, line_to_add)
        }

        if (input$process == "Normalize") {
            target_name <- paste0("normalized_", step_counter$counter)
            line_to_add <- paste0("normalize(", target_name, ", ", previous_target_name$name, ", factor_col = group_col, sample_id_col = sample_id_col, rowNorm = '", input$rowNormMethod, "', transNorm = '", input$transNormMethod, "', scaleNorm = '", input$scaleNormMethod, "', ref = '", input$ref, "', out_dir = out_dir),")

            # Append line to reactiveValues
            step_lines$lines <- c(step_lines$lines, line_to_add)
        }

        if (input$process == "Batch Correct") {
            target_name <- paste0("batch_corrected_", step_counter$counter)
            line_to_add <- paste0("batch_correct(", target_name, ", ", previous_target_name$name, ", method = '", input$batchCorrectMethod, "', out_dir = out_dir),")

            # Append line to reactiveValues
            step_lines$lines <- c(step_lines$lines, line_to_add)
        }

        previous_target_name$name <- target_name
        step_counter$counter <- step_counter$counter + 1
    })

    # Remove last step
    observeEvent(input$removeStep, {
        # Check if there are steps to remove
        if (length(step_lines$lines) > 0) {
            # Remove the last step added
            step_lines$lines <- step_lines$lines[-length(step_lines$lines)]
            # Decrement the step counter if greater than 1
            if (step_counter$counter > 1) {
                step_counter$counter <- step_counter$counter - 1
            }
        }
    })

    observeEvent(input$writeSteps, {
        # Transform the outDir to a valid path
        out_dir <- parseDirPath(directories, input$outDir)

        # Save the _targets.R file on the out_dir
        withr::with_dir(out_dir, {


        # Write lines to file
        targets_file <- targets_template
        template_lines <- readLines(targets_file)
        # Add the lines to the beginning of the file
        global_var_lines <- c(
            "#### Global variables #####",
            paste0("out_dir <- '", out_dir, "'"),
            paste0("dataMatrixPath <- '", data$dataMatrixPath, "'"),
            paste0("sampleMetadataPath <- '", data$sampleMetadataPath, "'"),
            paste0("variableMetadataPath <- '", data$variableMetadataPath, "'"),
            paste0("dataSep <- '", data$dataSeparator, "'"),
            paste0("sampleSep <- '", data$sampleSeparator, "'"),
            paste0("variableSep <- '", data$variableSeparator, "'"),
            "",
            "# Columns settings",
            paste0("factor_cols <- c(", paste0("'", data$factorCols, "'", collapse = ", "), ")"),
            paste0("sample_id_col <- '", data$sampleIdCol, "'"),
            paste0("sample_type_col <- '", data$sampleTypeCol, "'"),
            paste0("group_col <- '", data$groupCol, "'"),
            "",
            "",
            "dir.create(out_dir, showWarnings = FALSE)",
            "out_dir <- tools::file_path_as_absolute(out_dir)"
        )

        pipeline_load_lines <- c(
            "##### DEFINE THE PIPELINE ######",
            "list(",
            "# LOAD THE DATA",
            "load_data(data, dataMatrixPath, sampleMetadataPath, dataSep = dataSep, sampleSep = sampleSep, variableSep = variableSep),",
            "",
            "# Create a DatasetExperiment object",
            "createExperiment(experiment, data),",
            "",
            "# Factorize the cols",
            "factorize_cols(factorized, experiment, factor_cols),",
            "",
            "#SHINY STEPS",
            ""
        )

        pipeline_ending_lines <- c(
            "#### EXTRACTION ####",
            "# Extract the data",
            paste0("tar_target(extract_data, export_data(", previous_target_name$name, ", out_dir = out_dir, out_name = 'Processed')),"),
            "",
            "",
            "#### Cleaning ####",
            "tar_target(clean, withr::with_dir(out_dir, unlink('TempData', recursive = TRUE)))",
            ")"
        )

        lines <- c(template_lines, global_var_lines, pipeline_load_lines, step_lines$lines, pipeline_ending_lines)

        writeLines(lines, "_targets.R")
        })
    })

    volumes <- getVolumes()()

    directories <- c(wd = ".", dirUp = "..", volumes)
    # Function to choose output directory
    shinyDirChoose(input, "outDir", roots = directories, filetypes = c("", "txt"))


    observeEvent(input$runPipeline, {
        validate(
            need(data$dataMatrixPath != "", "Please upload the data matrix file."),
            need(data$sampleMetadataPath != "", "Please upload the sample metadata file."),
            need(input$outDir != "", "Please select an output directory."),
            need(data$factorCols != "", "Please select the factor columns."),
            need(data$sampleIdCol != "", "Please select the sample ID column."),
            need(data$sampleTypeCol != "", "Please select the sample type column."),
            need(data$groupCol != "", "Please select the group column.")
        )
        # Run the pipeline with error handling
        out_dir <- parseDirPath(directories, input$outDir)
        withr::with_dir(out_dir, {

        tryCatch(
            {
                library(targets)
                tar_make()
            },
            error = function(e) {
                # Print the error message
                print(paste("Error:", e$message))
                # Display a notification or message to the user
                showModal(modalDialog(
                    title = "Error",
                    "An error occurred. Please check your inputs and try again.",
                    easyClose = TRUE
                ))
            }
        )
        })
    })
}

----
Work\inst\shiny\ui.R
library(shiny)
library(DT)
library(shinyFiles)

navbarPage(
    id = "DataInitializer",
    title = "MetaboPipe Data Initializer",
    tabPanel(
        "Data Upload",
        sidebarLayout(
            sidebarPanel(
                selectInput("dataset", "Select Dataset:",
                    choices = c("MTBLS79", "ST000284", "Upload data"),
                    selected = "Upload data"
                ),
                conditionalPanel(
                    condition = "input.dataset == 'Upload data'",
                    fileInput("dataMatrix", "Choose DataMatrix file",
                        accept = c(".csv")
                    ),
                    radioButtons("dataSep", "Separator",
                        choices = c(
                            Comma = ",",
                            Semicolon = ";"
                        ),
                        selected = ","
                    ),
                    tags$hr(),
                    fileInput("sampleMetadata", "Choose Sample Metadata file",
                        accept = c(".csv")
                    ),
                    radioButtons("sampleSep", "Separator",
                        choices = c(
                            Comma = ",",
                            Semicolon = ";"
                        ),
                        selected = ","
                    ),
                    tags$hr(),
                    fileInput("variableMetadata", "Choose Variable Metadata file",
                        accept = c(".csv")
                    ),
                    radioButtons("variableSep", "Separator",
                        choices = c(
                            Comma = ",",
                            Semicolon = ";"
                        ),
                        selected = ","
                    )
                ),
                actionButton("loadData", "Load Data", style = "color: #fff; background-color: #28a745; border-color: #28a745;")
            ),
            mainPanel()
        )
    ),
    tabPanel(
        "DataConfig",
        sidebarLayout(
            sidebarPanel(
                selectizeInput("factorCols", "Select columns as factor variables:",
                    choices = NULL,
                    multiple = TRUE
                ),
                selectizeInput("sampleIdCol", "Select sample ID column:",
                    choices = NULL
                ),
                selectizeInput("sampleTypeCol", "Select sample Type column (if its QC, Blank or Sample):",
                    choices = NULL
                ),
                selectizeInput("groupCol", "Select the Group column (the study variable):",
                    choices = NULL
                ),
                actionButton("setSettings", "Set Settings", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
            ),
            mainPanel(
                tabsetPanel(
                    tabPanel("Data Summary", DT::dataTableOutput("dataMatrixTable")),
                    tabPanel("Sample Metadata Summary", DT::dataTableOutput("sampleMetadataTable")),
                    tabPanel("Variable Metadata Summary", DT::dataTableOutput("variableMetadataTable"))
                )
            )
        )
    ),
    tabPanel(
        "Process Selector",
        sidebarLayout(
            sidebarPanel(
                selectInput("process", "Select the first step to perform:",
                    choices = c("Filter", "Impute", "Normalize", "Batch Correct")
                ),
                conditionalPanel(
                    condition = "input.process == 'Filter'",
                    numericInput("naThreshold", "Select the threshold for missing values (between 0 and 1):",
                        min = 0, max = 1, value = 0.80, step = 0.01
                    ),
                    checkboxInput("filterOutliers", "Filter Outliers"),
                    conditionalPanel(
                        condition = "input.filterOutliers == true",
                        selectInput("confLimit", "Select the confidence limit for outlier removal:",
                            choices = c("0.95", "0.99")
                        )
                    )
                ),
                conditionalPanel(
                    condition = "input.process == 'Impute'",
                    selectInput("imputeMethod", "Select the imputation method:",
                        choices = c("mean", "median", "RF", "QRILC", "kNN", "SVD", "bpca", "ppca"),
                        selected = "RF"
                    ),
                    conditionalPanel(
                        condition = "input.imputeMethod == 'kNN'",
                        numericInput("k", "Select the number of neighbors for kNN imputation:",
                            min = 1, value = 5
                        )
                    ),
                    conditionalPanel(
                        condition = "input.imputeMethod == 'SVD' || input.imputeMethod == 'BPCA' || input.imputeMethod == 'PPCA'",
                        numericInput("k", "Select the number of principal components for imputation:",
                            min = 1, value = 5
                        )
                    )
                ),
                conditionalPanel(
                    condition = "input.process == 'Normalize'",
                    selectInput("rowNormMethod", "Select the row-wise normalization method:",
                        choices = c("QuantileNorm", "CompNorm", "SumNorm", "MedianNorm", "SpecNorm", "NULL"),
                        selected = "NULL"
                    ),
                    conditionalPanel(
                        condition = "input.rowNormMethod == 'CompNorm'",
                        selectizeInput("ref", "Enter the compound of reference for CompNorm normalization:",
                            choices = NULL
                        )
                    ),
                    selectInput("transNormMethod", "Select the transformation method:",
                        choices = c("LogNorm", "CrNorm", "NULL"),
                        selected = "NULL"
                    ),
                    selectInput("scaleNormMethod", "Select the scaling method:",
                        choices = c("MeanCenter", "AutoNorm", "ParetoNorm", "RangeNorm", "NULL"),
                        selected = "NULL"
                    )
                ),
                conditionalPanel(
                    condition = "input.process == 'Batch Correct'",
                    selectInput("batchCorrectMethod", "Select the batch correction method:",
                        choices = c("ComBat", "SVA", "RUV", "NULL"),
                        selected = "NULL"
                    ),
                    selectizeInput("orderCol", "Select the Order column (the injection order):",
                        choices = NULL
                    ),
                    selectizeInput("batchCol", "Select the batch column:",
                        choices = NULL
                    ),
                ),
                actionButton("removeStep", "Remove Step", style = "color: #fff; background-color: #dc3545; border-color: #dc3545;"),
                actionButton("addStep", "Add Step", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
            ),
            mainPanel(
                h3("Added Steps"),
                verbatimTextOutput("addedSteps"),
                shinyDirButton("outDir", "Output Directory", "Select a directory", "Choose", icon = icon("folder", class = "solid", lib = "font-awesome")),
                actionButton("writeSteps", "Write Steps", style = "color: #fff; background-color: #28a745; border-color: #28a745;"),
                actionButton("runPipeline", "Run Pipeline", style = "color: #fff; background-color: #007bff; border-color: #007bff;")
            )
        )
    )
)

----
Work\vignettes\.gitignore
_targets/
results

Rplots.pdf
----
Work\vignettes\_targets.R
# _targets.R file:
library(targets)
library(tarchetypes)
library(crew)
library(metaboPipe)
# library(doParallel)
# Load all R scripts in the R/ directory.
# file.sources <- list.files("R", pattern = "*.R", full.names = TRUE)
# invisible(sapply(file.sources, source, .GlobalEnv))

# Declare libraries
# These are the libraries that the pipeline depends on.

# To create a list with all the libraries to copy-paste:
# cat(paste(shQuote(unique(renv::dependencies(path = "R")$Package), type="cmd"), collapse=", "))

tar_option_set(
  packages = c(
    "structToolbox", "SummarizedExperiment", "VIM", "impute", "imputeLCMD",
    "missForest", "caret", "pcaMethods", "tidyverse", "MetaboAnalystR", "tinytex",
    "HotellingEllipse", "ggforce", "tools", "cowplot"
  )
)

# tar_source()
# Declare controller
# Create a controller with 5 workers and a 3-second idle time.
# controller <- crew::crew_controller_local(
#   name = "Controller",
#   workers = 2,
#   seconds_idle = 3
# )
# tar_option_set(controller = controller)
#### Global variables #####
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")
factor_col = "Groups"
sample_id_col = "sample_id"


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5),
  normalize(normalized_experiment, imputed_experiment, factor_col = factor_col, sample_id_col = sample_id_col, rowNorm = 'CompNorm', ref = 'Creatine (132.1 / 90.0)', out_dir = outdir),
  normalize(scaled_experiment, normalized_experiment,factor_col = factor_col, sample_id_col = sample_id_col, scaleNorm = "AutoNorm", out_dir = outdir),
  normalize(transformed_experiment, scaled_experiment, factor_col = factor_col, sample_id_col = sample_id_col, transNorm = "LogNorm", out_dir= outdir),
  exportData(export, transformed_experiment, out_dir= outdir)
)

----
Work\vignettes\my-vignette.Rmd
---
title: "Data Processing of metabolomics datasets using metaboPipe"
author: "Perez Mendez, Eduard"
date: "`r Sys.Date()`"
output: 
  BiocStyle::html_document:
      toc: true
      toc_float: true
      toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Data Processing of metabolomics datasets using metaboPipe}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy.opts = list(width.cutoff = 60), 
  tidy = TRUE,
  dpi=96,fig.width=5,fig.height=5.5,fig.retina = 1,fig.small = TRUE
)

set.seed(57475)
```


# Introduction

Metabolomics, the study of small molecules in biological systems, plays a crucial role in understanding various physiological processes and disease mechanisms. As the volume and complexity of metabolomics data continue to grow, there is an increasing demand for robust and efficient data analysis pipelines.

`metaboPipe` is a comprehensive R package designed to streamline metabolomics analysis workflows. Built with the aim of simplifying data preprocessing tasks. `metaboPipe` offers a suite of tools tailored specifically for metabolomics researchers.

In this vignette, we will explore the key functionalities of `metaboPipe` and demonstrate how it can be used to perform common metabolomics preprocessing tasks. From loading the data, filtering, imputing, normalizing, and scaling, to extracting the processed data, `metaboPipe` provides a user-friendly interface to facilitate seamless integration into your metabolomics workflow.


## Installation

Clone the `metaboPipe` repository from GitHub and install the package using the following commands:
```{r installation, eval=FALSE}
# Install the package
install.packages("devtools")
devtools::install_github("https://github.com/eperezme/metaboPipe", subdir = "Work")
```

## Introduction to `metaboPipe` and its functionalities
For our example we will load the ST000284 dataset. `EXPLAIN THE DATASET`

```{r}
DE <- metaboPipe::ST000284
DE
```


`metaboPipe` uses the `DatasetExperiment` object from the `structToolbox` package as the main **data structure** for storing and manipulating metabolomics data. The `DatasetExperiment` object is a list of data frames that contains the following components: 


- `data`: This encapsulates a data frame housing the measured data pertaining to each sample.
- `sample_meta`: This comprises a data frame furnishing supplementary sample-related information, such as group labels, often referred to as *phenoData.*
- `variable_meta`: This includes a data frame presenting additional details concerning variables (features), such as annotations.

Similar to all `struct` entities, it encompasses `name` and `description` fields, referred to as "slots" within the R language.

A notable distinction between `DatasetExperiment` and `SummarizedExperiment` structures lies in the data orientation. In `DatasetExperiment` instances, **samples are arranged in rows** while **features occupy columns**, contrasting with the arrangement in `SummarizedExperiment` structures.

All slots are accessible using dollar notation.

```{r, eval=FALSE}
head(DE$data[, 1:4])
head(DE$sample_meta[, 1:4])
head(DE$variable_meta)
```



# Creating a pipeline 

`metaboPipe` uses the `targets` package to define and execute data processing pipelines. The pipeline is defined as a list of targets, each representing a step in the data processing workflow. Each target is a function call that takes input data and produces output data. The `targets` package ensures that each target is executed only when its dependencies are met, allowing for efficient and reproducible data processing.

The `metaboPipe` pipeline functions consists of several functions that create targets to execute specific processing steps, including data loading, filtering, batch correction, imputation and normalization (that includes tranformation and scaling). Each step is defined as a single or multiple targets in the pipeline, and the targets are executed sequentially to process the data.

In order to use the pipeline functions in `metaboPipe`, we need to create a targets file. You can use the `use_targets()` function to create a new `_targets.R` file in the working directory or use the `metaboPipe` template:
```{r, eval=FALSE}
create_pipeline()
```

An example `_targets.R` file is created and saved in the working directory. The `_targets.R` file contains some example code, which we will modify to define the pipeline for our data processing workflow.
First of all we will load the required packages for `metaboPipe` and set the target options in the `_targets.R` file.

```{r, eval=FALSE}
# Load packages required to define the pipeline:
library(targets)
library(tarchetypes)
library(metaboPipe)

tar_option_set(
  packages = c(
    "structToolbox", "SummarizedExperiment", "VIM", "impute", "imputeLCMD",
    "missForest", "caret", "pcaMethods", "tidyverse", "MetaboAnalystR", "tinytex",
    "HotellingEllipse", "ggforce", "tools", "cowplot", "metaboPipe"
  )
)
```

Now that the packages for our pipeline are set up, we can define the pipeline workflow. The pipeline consists of several steps, including data loading, filtering, batch correction, imputation, and normalization. Each step is defined as a function in the script but those functions create multiple targets, and the targets are executed sequentially to process the data.

The targets should be defined inside a `list()` function. 
We can load the data using the `tar_target()` function, which defines a target that loads the data from the specified file path.
We first will load the data using the `tar_target()` function, which defines a target that reads the data from the specified file path. And with the 3 dataframes loaded we can create another target to merge them into a `DatasetExperiment` object.

The first variable on the `tar_target()` function is the name of the target, that will be used for refering the objects on subsequent targets and the second variable is the code that will be executed to generate the target.
```{r, eval=FALSE}
list(
  tar_target(dataMatrix, read.csv("data/ST000284/dataMatrix.csv")),
  tar_target(sampleMeta, read.csv("data/ST000284/sampleMetadata.csv")),
  tar_target(variableMetadata, read.csv("data/ST000284/variableMetadata.csv")),
  tar_target(experiment, warper_createExperiment(dataMatrix, sampleMeta, variableMetadata, experiment_name = "ST000284", experiment_description = "Example metabolomics dataset ST000284"))
)
```

If then we execute `tar_make()` the pipeline will be executed and the data will be loaded and merged into a `DatasetExperiment` object. To load the object into the R environment we can use the `tar_load()` function.

```{r, eval=FALSE}
tar_load(experiment)
```


Even though the pipeline is correctly defined, `metaboPipe` provides a set of functions to make the pipeline definition easier.

## Using metaboPipe functions to define the pipeline
### Loading the data

Instead of writing 3 targets to load the 3 dataframes we can define the paths to the files before the targets list and then use the `load_data()` function to load the 3 dataframes. And then we can use the `createExperiment()` function to merge the 3 dataframes into a `DatasetExperiment` object.

The `load_data()` function takes the paths to the data files as arguments and returns a list with the loaded dataframes. The `createExperiment()` function takes the loaded dataframes as arguments and returns a `DatasetExperiment` object.

Same as before, the first variable on the functions is the name of the target.
```{r load_data_exp, eval=FALSE}
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"

list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath),
  createExperiment(experiment, data_loaded, experiment_name = "ST000284", experiment_description = "Example metabolomics dataset ST000284")
)
```

Now if we run `tar_make()` the pipeline will be executed and the data will be loaded and merged into a `DatasetExperiment` object. As before to load the object into the R environment we can use the `tar_load()` function.
```{r load, eval=FALSE}
tar_load(experiment)
```

By default, `load_data()` uses the "," as the separator for the data files. If the data files use a different separator, we can define for each file the separator:
```{r load_separators, eval=FALSE}
# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description)
)
```

## Factorizing the columns of the experiment
There are functions that assume that specific columns are factors, such as the `normalize()` function. To factorize the columns of the experiment we can use the `factorize_cols()` function. This function takes the `DatasetExperiment` object, and a vector of column names as an argument and returns the object with the columns factorized.
```{r factorize, eval=FALSE}
# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns)
)
```

## Filtering the data
After the setup of the experiment, we can start the data manipulation steps. The first step we will perform is to filter the data. The `filter_step()` function encompases 2 filtering steps: 1) filtering by % of missing values and 2) filtering the outliers. The threshold is defined as the minimum percentage of data explained. By default the `threshold` is `0.8`, meaning that each row and collumn must have at least 80% of the values or get removed. The `filter_outliers` argument is a boolean that defines if the outliers should be removed or not and the `conf.limit` argument specify the confidence interval (either `'0.95'` or `'0.99'`) to discriminate the observation as an outlier or not. 

```{r filter, eval=FALSE}
# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = "results")
)
```

The `out_dir` option is where the plots will be exported under the path: `out_dir/plots`, in this case: `results/plots`.

## Imputing
The next step is to impute the missing values that remain after the filter step. To perform that we use the `impute()` function specifying the impute method we want to use and if it needs a special parameter like knn we specify it using `k`. In this example we will use a `Random forest` approach.
```{r impute, eval=FALSE}
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5) # Impute using Random forest
)
```


## Normalizing
Next step is to normalize the data. To do so we use the `normalize()` function, that uses MetaboAnalystR to normalize the data. To normalize the data we need to specify the factor study column and the column with the sample id using `factor_col`  and the `sample_id_col` arguments. Also the method of normalization (by rows) with the `rowNorm` wich in this case is by internal compound `CompNorm`. We specify the internal compound name with `ref`. 

```{r normalize, eval=FALSE}
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5),
  normalize(normalized_experiment, imputed_experiment, factor_col = "Groups", sample_id_col = "sample_id", rowNorm = 'CompNorm', ref = 'Creatine..132.1...90.0.', out_dir = outdir)
)

```


## Scaling
Scaling is also applied using the `normalize()` function. But to do that, instead of the `rowNorm` we specify it using the `scaleNorm` argument. We defined in the preamble the factor_col and sample_id_col for simplicity.

```{r scaling, eval=FALSE}
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")
factor_col = "Groups"
sample_id_col = "sample_id"


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5),
  normalize(normalized_experiment, imputed_experiment, factor_col = factor_col, sample_id_col = sample_id_col, rowNorm = 'CompNorm', ref = 'Creatine..132.1...90.0.', out_dir = outdir),
  normalize(scaled_experiment, normalized_experiment,factor_col = factor_col, sample_id_col = sample_id_col, scaleNorm = "AutoNorm", out_dir = outdir)
)
```

## Transformation
Transforming the data is also made with the `normalize()` function under the argument `transNorm`.

```{r transform, eval=FALSE}
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")
factor_col = "Groups"
sample_id_col = "sample_id"


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5),
  normalize(normalized_experiment, imputed_experiment, factor_col = factor_col, sample_id_col = sample_id_col, rowNorm = 'CompNorm', ref = 'Creatine (132.1 / 90.0)', out_dir = outdir),
  normalize(scaled_experiment, normalized_experiment,factor_col = factor_col, sample_id_col = sample_id_col, scaleNorm = "AutoNorm", out_dir = outdir),
  normalize(transformed_experiment, scaled_experiment, factor_col = factor_col, sample_id_col = sample_id_col, transNorm = "LogNorm", out_dir= outdir)
)
```

## Extracting the data
Now that we have defined all the steps that we want the pipeline to perform we should tell them to extract the data processed. This can be achieved using the `export_data()` function:

```{r export, eval=FALSE}
# General config
outdir = "results"
dir.create(outdir, showWarnings = FALSE) # We create the outdir in case there its not created yet
outdir <- tools::file_path_as_absolute(outdir) # We get the absolute path of the dir for compatibility

# Load the Data
dataMatrixPath <- "data/ST000284/dataMatrix.csv"
sampleMetadataPath <- "data/ST000284/sampleMetadata.csv"
variableMetadataPath <- "data/ST000284/variableMetadata.csv"
dataSep <- ","
sampleSep <- ","
variableSep <- ","

# Create the experiment
name <- "ST000284"
description <- "Example metabolomics dataset ST000284"

# Setting up the columns
columns <- c("Groups", "Age", "Gender", "Smoking", "Alcohol")
factor_col = "Groups"
sample_id_col = "sample_id"


list(
  load_data(data_loaded, dataMatrixPath, sampleMetadataPath, variableMetadataPath, dataSep, sampleSep, variableSep),
  createExperiment(experiment, data_loaded, experiment_name = name, experiment_description = description),
  factorize_cols(factorized_experiment, experiment, columns),
  filter_step(filtered_experiment, factorized_experiment, threshold = 0.8, filter_outliers = TRUE, conf.limit = "0.95", out_dir = outdir),
  impute(imputed_experiment, filtered_experiment, method = 'RF', k = 5),
  normalize(normalized_experiment, imputed_experiment, factor_col = factor_col, sample_id_col = sample_id_col, rowNorm = 'CompNorm', ref = 'Creatine (132.1 / 90.0)', out_dir = outdir),
  normalize(scaled_experiment, normalized_experiment,factor_col = factor_col, sample_id_col = sample_id_col, scaleNorm = "AutoNorm", out_dir = outdir),
  normalize(transformed_experiment, scaled_experiment, factor_col = factor_col, sample_id_col = sample_id_col, transNorm = "LogNorm", out_dir= outdir),
  exportData(export, transformed_experiment, out_dir= outdir)
)
```

## Running the pipeline

Now we are ready to run the pipeline. We can view a node map of the steps if we use the `tar_visnetwork()` function of the `targets` package. And we can view a full list of the steps with the function using `tar_manifest()`.
```{r tar_check, paged.print=FALSE}
targets::tar_visnetwork(targets_only = TRUE)
targets::tar_manifest()
```


An now, to run the pipeline we just have to:
```{r run_pipeline}
targets::tar_make()
```



# Using the Pilers 

Now that you know the core functions of `metaboPipe`, you can initialize and create the pipeline using the Shiny app for easiness.
In order to call the app:
```{r shiny, eval=FALSE}
metaboPipe::pipePilers()
```




# Session Info
```{r sessionInfo,}
sessionInfo()
```

--END--